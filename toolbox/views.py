import hashlib
import json
import logging
import os
import platform
import random
import re
import subprocess
import time
import traceback
from datetime import timedelta, datetime
from typing import List, Union

import aip
import cloudscraper
import dateutil.parser
import demjson3
import feedparser
import git
import jwt
import qbittorrentapi
import requests
import telebot
import toml as toml
import transmission_rpc
from django.conf import settings
from django.core.cache import cache
from django.shortcuts import get_object_or_404
from lxml import etree
from pypushdeer import PushDeer
from telebot import apihelper

import my_site
from auxiliary.base import DownloaderCategory
from auxiliary.settings import BASE_DIR
from configuration.models import PushConfig
from download.models import Downloader
from my_site.models import SiteStatus, TorrentInfo, MySite
from toolbox.schema import CommonResponse, DotDict
from website.models import WebSite
from . import pushplus
from .cookie_cloud import CookieCloudHelper
from .wechat_push import WechatPush
from .wxpusher import WxPusher

# Create your views here.
logger = logging.getLogger('ptools')


def parse_toml(cmd) -> dict:
    """ä»é…ç½®æ–‡ä»¶è§£æè·å–ç›¸å…³é¡¹ç›®"""
    try:
        data = toml.load('db/ptools.toml')
        return data.get(cmd, {})
    except Exception as e:
        return dict()


def check_token(token) -> bool:
    try:
        own_token = parse_toml('token').get('token')
        logger.info(f'{own_token}=={token}')
        return own_token == token
    except Exception as e:
        logger.error(e)
        return False


def cookie2dict(source_str: str) -> dict:
    """
    cookieså­—ç¬¦ä¸²è½¬ä¸ºå­—å…¸æ ¼å¼,ä¼ å…¥å‚æ•°å¿…é¡»ä¸ºcookieså­—ç¬¦ä¸²
    """
    if not source_str:
        return {}
    dist_dict = {}
    list_mid = source_str.strip().split(';')
    for i in list_mid:
        # ä»¥ç¬¬ä¸€ä¸ªé€‰ä¸­çš„å­—ç¬¦åˆ†å‰²1æ¬¡ï¼Œ
        if len(i) <= 0:
            continue
        list2 = i.split('=', 1)
        dist_dict[list2[0].strip()] = list2[1].strip()
    return dist_dict


# è·å–å­—ç¬¦ä¸²ä¸­çš„æ•°å­—
get_decimals = lambda x: re.search("\d+(\.\d+)?", x).group() if re.search("\d+(\.\d+)?", x) else 0


class FileSizeConvert:
    """æ–‡ä»¶å¤§å°å’Œå­—èŠ‚æ•°äº’è½¬"""

    @staticmethod
    def parse_2_byte(file_size: str) -> int:
        if not file_size:
            return 0
        """å°†æ–‡ä»¶å¤§å°å­—ç¬¦ä¸²è§£æä¸ºå­—èŠ‚"""
        regex = re.compile(r'(\d+(?:\.\d+)?)\s*([kmgtp]?b)', re.IGNORECASE)

        order = ['b', 'kb', 'mb', 'gb', 'tb', 'pb', 'eb']

        for value, unit in regex.findall(file_size):
            return int(float(value) * (1024 ** order.index(unit.lower())))

    @staticmethod
    def parse_2_file_size(byte: int) -> str:
        if not byte:
            return '0B'
        units = ["B", "KB", "MB", "GB", "TB", "PB", 'EB']
        size = 1024.0
        for i in range(len(units)):
            if (byte / size) < 1:
                return "%.3f %s" % (byte, units[i])
            byte = byte / size


def baidu_ocr_captcha(img_url):
    """ç™¾åº¦OCRé«˜ç²¾åº¦è¯†åˆ«ï¼Œä¼ å…¥å›¾ç‰‡URL"""
    # è·å–ç™¾åº¦è¯†åˆ«ç»“æœ
    ocr = parse_toml("ocr")
    # ocr = BaiduOCR.objects.filter(enable=True).first()
    if not ocr:
        logger.error('æœªè®¾ç½®ç™¾åº¦OCRæ–‡æœ¬è¯†åˆ«APIï¼Œæ— æ³•ä½¿ç”¨æœ¬åŠŸèƒ½ï¼')
        return CommonResponse.error(msg='æœªè®¾ç½®ç™¾åº¦OCRæ–‡æœ¬è¯†åˆ«APIï¼Œæ— æ³•ä½¿ç”¨æœ¬åŠŸèƒ½ï¼')
    try:
        ocr = DotDict(ocr)
        ocr_client = aip.AipOcr(appId=ocr.app_id, secretKey=ocr.secret_key, apiKey=ocr.api_key)
        res1 = ocr_client.basicGeneralUrl(img_url)
        logger.info(res1)
        if res1.get('error_code'):
            res1 = ocr_client.basicAccurateUrl(img_url)
        logger.info('res1: {}'.format(res1))
        if res1.get('error_code'):
            return CommonResponse.error(msg=res1.get('error_msg'))
        res2 = res1.get('words_result')[0].get('words')
        # å»é™¤æ‚ä¹±å­—ç¬¦
        imagestring = ''.join(re.findall('[A-Za-z0-9]+', res2)).strip()
        logger_info = 'ç™¾åº¦OCRå¤©ç©ºéªŒè¯ç ï¼š{}ï¼Œé•¿åº¦ï¼š{}'.format(imagestring, len(imagestring))
        logger.info(logger_info)
        # è¯†åˆ«é”™è¯¯å°±é‡æ¥

        return CommonResponse.success(data=imagestring)
    except Exception as e:
        msg = 'ç™¾åº¦OCRè¯†åˆ«å¤±è´¥ï¼š{}'.format(e)
        logger.info(traceback.format_exc(limit=3))
        # raise
        # self.send_text(title='OCRè¯†åˆ«å‡ºé”™å’¯', message=msg)
        return CommonResponse.error(msg=msg)


def parse_school_location(text: list):
    logger.info('è§£æå­¦æ ¡è®¿é—®é“¾æ¥ï¼š{}'.format(text))
    list1 = [x.strip().strip('"') for x in text[0].split('+')]
    list2 = ''.join(list1).split('=', 1)[1]
    return list2.strip(';').strip('"')


def parse_message_num(messages: str):
    """
    è§£æç½‘ç«™æ¶ˆæ¯æ¡æ•°
    :param messages:
    :return:
    """
    list1 = messages.split('(')
    if len(list1) > 1:
        count = re.sub(u"([^(\u0030-\u0039])", "", list1[1])
    elif len(list1) == 1:
        count = messages
    else:
        count = 0
    return int(count)


def generate_config_file():
    file_path = os.path.join(BASE_DIR, 'db/ptools.toml')
    try:
        if not os.path.exists(file_path):
            with open(file_path, 'w') as toml_f:
                toml_f.write('')
                toml.dump({}, toml_f)
                logger.info(f'é…ç½®æ–‡ä»¶ç”ŸæˆæˆåŠŸï¼')
                return CommonResponse.success(
                    msg='é…ç½®æ–‡ä»¶ç”ŸæˆæˆåŠŸï¼',
                )
        return CommonResponse.success(msg='é…ç½®æ–‡ä»¶æ–‡ä»¶å·²å­˜åœ¨ï¼', )
    except Exception as e:
        return CommonResponse.error(msg=f'åˆå§‹åŒ–å¤±è´¥ï¼{e}', )


def exec_command(commands):
    """æ‰§è¡Œå‘½ä»¤è¡Œå‘½ä»¤"""
    result = []
    for key, command in commands.items():
        p = subprocess.run(command, shell=True)
        logger.info('{} å‘½ä»¤æ‰§è¡Œç»“æœï¼š\n{}'.format(key, p))
        result.append({
            'command': key,
            'res': p.returncode
        })
    return result


def verify_token():
    token = os.getenv("TOKEN", None)
    if not token:
        return 'æ‚¨çš„è½¯ä»¶æœªç»æˆæƒï¼Œå¦‚æœæ‚¨å–œæ¬¢æœ¬è½¯ä»¶ï¼Œæ¬¢è¿ä»˜è´¹è´­ä¹°æˆæƒæˆ–ç”³è¯·ä¸´æ—¶æˆæƒã€‚'
    if os.path.exists(f"db/encrypted_key.bin"):
        res = subprocess.run([f"encrypt_tool/{platform.uname().machine}/main.bin"], stdout=subprocess.PIPE)
        res_json = json.loads(res.stdout)
        if res_json['code'] == 0:
            return res_json['msg']
        else:
            try:
                result = subprocess.run(['supervisorctl', 'stop', 'celery-beat'], check=True, text=True,
                                        capture_output=True)
                logger.debug(f'Successfully executed command: {result.stdout}')
            except Exception as e:
                logger.debug(f'Failed executed command')

            return 'æ‚¨çš„è½¯ä»¶æœªç»æˆæƒï¼Œå¦‚æœæ‚¨å–œæ¬¢æœ¬è½¯ä»¶ï¼Œæ¬¢è¿ä»˜è´¹è´­ä¹°æˆæƒæˆ–ç”³è¯·ä¸´æ—¶æˆæƒã€‚'
    res = requests.get('http://repeat.ptools.fun/api/user/verify', params={
        "token": token,
        "email": os.getenv("DJANGO_SUPERUSER_EMAIL", None)
    })
    if res.status_code == 200 and res.json().get('code') == 0:
        return res.json().get('msg')
    else:
        msg = f'æ‚¨çš„è½¯ä»¶æœªæˆæƒï¼Œæˆ–è¿æ¥æˆæƒæœåŠ¡å™¨å¤±è´¥ï¼{res.json().get("msg")}'
        logger.error(msg)
        subprocess.run(['supervisorctl', 'stop', 'celery-beat'], check=True, text=True, capture_output=True)
        return msg


def send_text(message: str, title: str = '', url: str = None):
    """é€šçŸ¥åˆ†æµ"""
    notifies = parse_toml("notify")
    if len(notifies) <= 0:
        msg = 'ä½ è¿˜æ²¡æœ‰é…ç½®é€šçŸ¥å‚æ•°å“¦ï¼'
        logger.warning(msg)
        return msg
    try:
        message = f'> {verify_token()}  \n\n{message}'
        pass
    except Exception as e:
        msg = f'æˆæƒéªŒè¯å¤±è´¥ï¼'
        logger.error(msg)
        logger.error(traceback.format_exc(5))
        return msg
    for key, notify in notifies.items():
        try:
            if key == PushConfig.wechat_work_push:
                """ä¼ä¸šå¾®ä¿¡é€šçŸ¥"""
                server = notify.get('server', 'https://qyapi.weixin.qq.com/')
                if not server.endswith('/'):
                    server = server + '/'
                notify_push = WechatPush(
                    corp_id=notify.get('corp_id'),
                    secret=notify.get('corpsecret'),
                    agent_id=notify.get('agent_id'),
                    server=server,
                )
                max_length = 2000  # æœ€å¤§æ¶ˆæ¯é•¿åº¦é™åˆ¶
                if len(message) <= max_length:
                    res = notify_push.send_text(
                        text=message,
                        to_uid=notify.get('to_uid', '@all')
                    )
                else:
                    res = ''
                    while message:
                        chunk = message[:max_length]  # ä»æ¶ˆæ¯ä¸­æˆªå–æœ€å¤§é•¿åº¦çš„éƒ¨åˆ†
                        res = notify_push.send_text(
                            text=chunk,
                            to_uid=notify.get('to_uid', '@all')
                        )
                        message = message[max_length:]  # å‰©ä½™éƒ¨åˆ†ä½œä¸ºæ–°çš„æ¶ˆæ¯è¿›è¡Œä¸‹ä¸€è½®å‘é€
                        logger.info(res)
                msg = f'ä¼ä¸šå¾®ä¿¡é€šçŸ¥ï¼š{res}'
                logger.info(msg)

            if key == PushConfig.wxpusher_push:
                """WxPusheré€šçŸ¥"""
                res = WxPusher.send_message(
                    summary=title,
                    content=message,
                    url=url,
                    uids=notify.get('uids').split(','),
                    token=notify.get('token'),
                    content_type=3,  # 1ï¼šæ–‡æœ¬ï¼Œ2ï¼šhtmlï¼Œ3ï¼šmarkdown
                )
                msg = 'WxPusheré€šçŸ¥{}'.format(res)
                logger.info(msg)

            if key == PushConfig.pushdeer_push:
                pushdeer = PushDeer(
                    server=notify.get('custom_server', 'https://api2.pushdeer.com'),
                    pushkey=notify.get('pushkey')
                )
                # res = pushdeer.send_text(text, desp="optional description")
                res = pushdeer.send_markdown(text=message,
                                             desp=title)
                msg = 'pushdeeré€šçŸ¥{}'.format(res)
                logger.info(msg)

            if key == PushConfig.bark_push:
                res = requests.post(
                    url=f'{notify.get("custom_server", "https://api.day.app/")}push',
                    data={
                        'title': title,
                        'body': message,
                        'device_key': notify.get("device_key"),
                        # 'url': 'http://img.ptools.fun/pay.png',
                        'icon': 'https://gitee.com/ngfchl/ptools/raw/master/static/logo4.png'
                    },
                )
                msg = 'barké€šçŸ¥ {}'.format(res.json())
                logger.info(msg)

            if key == PushConfig.iyuu_push:
                url = notify.get("custom_server", 'http://iyuu.cn/') + '{}.send'.format(notify.get("token"))
                # text = '# '
                res = requests.post(
                    url=url,
                    data={
                        'text': title,
                        'desp': message
                    })
                msg = f'çˆ±è¯­é£é£é€šçŸ¥ï¼š{res}'
                logger.info(msg)

            if key == PushConfig.telegram_push:
                """Telegramé€šçŸ¥"""
                telegram_token = notify.get('telegram_token')
                telegram_chat_id = notify.get('telegram_chat_id')
                bot = telebot.TeleBot(telegram_token)
                proxy = notify.get('proxy')
                if proxy:
                    apihelper.proxy = proxy
                max_length = 4096  # æœ€å¤§æ¶ˆæ¯é•¿åº¦é™åˆ¶
                parse_mode = notify.get('parse_mode') if notify.get('parse_mode') else "HTML"
                if len(message) <= max_length:
                    bot.send_message(telegram_chat_id, message, parse_mode=parse_mode)  # å¦‚æœæ¶ˆæ¯é•¿åº¦ä¸è¶…è¿‡æœ€å¤§é™åˆ¶ï¼Œç›´æ¥å‘é€æ¶ˆæ¯
                else:
                    while message:
                        chunk = message[:max_length]  # ä»æ¶ˆæ¯ä¸­æˆªå–æœ€å¤§é•¿åº¦çš„éƒ¨åˆ†
                        bot.send_message(telegram_chat_id, chunk, parse_mode=parse_mode)  # å‘é€æ¶ˆæ¯éƒ¨åˆ†
                        message = message[max_length:]  # å‰©ä½™éƒ¨åˆ†ä½œä¸ºæ–°çš„æ¶ˆæ¯è¿›è¡Œä¸‹ä¸€è½®å‘é€

                msg = 'Telegramé€šçŸ¥æˆåŠŸ'
                logger.info(msg)
            if key == PushConfig.pushplus:
                token = notify.get('token')
                template = notify.get('template') if notify.get('template') else "markdown"
                res = pushplus.send_text(token=token, title=title, content=message, template=template)
                logger.info(res)
        except Exception as e:
            msg = f'é€šçŸ¥å‘é€å¤±è´¥ï¼Œ{traceback.format_exc(limit=5)}'
            logger.error(msg)


def get_git_log(branch='master', n=5):
    repo = git.Repo(path='.')
    # æ‹‰å–ä»“åº“æ›´æ–°è®°å½•å…ƒæ•°æ®
    repo.remote().set_url('git@gitee.com:ngfchl/auxiliary.git')
    repo.git.config('core.sshCommand', f'ssh -i /root/.ssh/id_rsa')
    repo.remote().fetch()
    # commitsæ›´æ–°è®°å½•
    logger.info('å½“å‰åˆ†æ”¯{}'.format(branch))
    return [{
        'date': log.committed_datetime.strftime('%Y-%m-%d %H:%M:%S'),
        'data': log.message,
        'hex': log.hexsha[:16],
    } for log in list(repo.iter_commits(branch, max_count=n))]


def today_data():
    """è·å–å½“æ—¥ç›¸è¾ƒäºå‰ä¸€æ—¥ä¸Šä¼ ä¸‹è½½æ•°æ®å¢é•¿é‡"""
    today_site_status_list = SiteStatus.objects.filter(created_at__date=datetime.today())
    increase_info_list = []
    total_upload = 0
    total_download = 0
    for site_state in today_site_status_list:
        my_site = site_state.site
        yesterday_site_status_list = SiteStatus.objects.filter(site=my_site)
        if len(yesterday_site_status_list) >= 2:
            yesterday_site_status = SiteStatus.objects.filter(site=my_site).order_by('-created_at')[1]
            uploaded_increase = site_state.uploaded - yesterday_site_status.uploaded
            downloaded_increase = site_state.downloaded - yesterday_site_status.downloaded
        else:
            uploaded_increase = site_state.uploaded
            downloaded_increase = site_state.downloaded
        if uploaded_increase + downloaded_increase <= 0:
            continue
        total_upload += uploaded_increase
        total_download += downloaded_increase
        increase_info_list.append({
            'name': my_site.nickname,
            'uploaded': uploaded_increase,
            'downloaded': downloaded_increase
        })
    increase_info_list.sort(key=lambda x: x.get('uploaded'), reverse=True)
    return total_upload, total_download, increase_info_list


def get_token(payload, timeout):
    salt = settings.SECRET_KEY
    payload["exp"] = datetime.utcnow() + timedelta(minutes=timeout)  # è®¾ç½®åˆ°æœŸæ—¶é—´
    # token = jwt.encode(payload=payload, key=salt, headers=headers).decode("utf-8")
    token = jwt.encode(payload=payload, key=salt, algorithm="HS256")
    return token


def parse_ptpp_cookies(data_list):
    # è§£æå‰ç«¯ä¼ æ¥çš„æ•°æ®
    datas = json.loads(data_list.cookies)
    info_list = json.loads(data_list.info)
    # userdata_list = json.loads(data_list.userdata)
    cookies = []
    try:
        for data, info in zip(datas, info_list):
            cookie_list = data.get('cookies')
            host = data.get('host')
            cookie_str = ''
            for cookie in cookie_list:
                cookie_str += '{}={};'.format(cookie.get('name'), cookie.get('value'))
            # logger.info(domain + cookie_str)
            cookies.append({
                'url': data.get('url'),
                'host': host,
                'icon': info.get('icon'),
                'info': info.get('user'),
                'passkey': info.get('passkey'),
                'cookies': cookie_str.rstrip(';'),
                # 'userdatas': userdata_list.get(host)
            })
        logger.info('ç«™ç‚¹è®°å½•å…±{}æ¡'.format(len(cookies)))
        # logger.info(cookies)
        return cookies
    except Exception as e:
        # raise
        # æ‰“å°å¼‚å¸¸è¯¦ç»†ä¿¡æ¯
        logger.error(traceback.format_exc(limit=3))
        send_text(title='PTPPç«™ç‚¹å¯¼å…¥é€šçŸ¥', message='Cookiesè§£æå¤±è´¥ï¼Œè¯·ç¡®è®¤å¯¼å…¥äº†æ­£ç¡®çš„cookieså¤‡ä»½æ–‡ä»¶ï¼')
        return 'Cookiesè§£æå¤±è´¥ï¼Œè¯·ç¡®è®¤å¯¼å…¥äº†æ­£ç¡®çš„cookieså¤‡ä»½æ–‡ä»¶ï¼'


def parse_rss(rss_url: str):
    """
    åˆ†æRSSè®¢é˜…ä¿¡æ¯
    :param rss_url:
    :return: è§£æå¥½çš„ç§å­åˆ—è¡¨
    """
    feed = feedparser.parse(rss_url)
    torrents = []
    for article in feed.entries:
        # logger.info(article.published).get('enclosure').get('url'))
        # logger.info(time.strftime('%Y-%m-%d %H:%M:%S', article.published_parsed))
        link = article.links[-1]
        torrents.append({
            'title': article.title[:255],
            'tid': (article.link.split('=')[-1]),
            'size': link.get('length'),
            'magnet_url': link.get('href'),
            'published': datetime.fromtimestamp(time.mktime(article.published_parsed)),
        })
    return torrents


def get_downloader_instance(downloader_id):
    """æ ¹æ®idè·å–ä¸‹è½½å®ä¾‹"""
    try:
        downloader = Downloader.objects.filter(id=downloader_id).first()
        if not downloader:
            msg = f"è¯·ç¡®è®¤å½“å‰æ‰€é€‰ä¸‹è½½å™¨æ˜¯å¦å­˜åœ¨ï¼"
            logger.error(msg)
            return None, msg, None
        if downloader.category == DownloaderCategory.qBittorrent:
            client = qbittorrentapi.Client(
                host=downloader.host,
                port=downloader.port,
                username=downloader.username,
                password=downloader.password,
                SIMPLE_RESPONSES=True,
                REQUESTS_ARGS={
                    'timeout': (5, 60)
                }
            )
            client.auth_log_in()
        else:
            client = transmission_rpc.Client(
                host=downloader.host, port=downloader.port,
                protocol=downloader.http,
                username=downloader.username,
                password=downloader.password,
                timeout=60,
            )
        return client, downloader.category, downloader.name
    except Exception as e:
        logger.error(traceback.format_exc(3))
        msg = f'ä¸‹è½½å™¨è¿æ¥å¤±è´¥ï¼š{e}'
        logger.exception(msg)
        return None, msg, None


def get_downloader_speed(downloader: Downloader):
    """è·å–å•ä¸ªä¸‹è½½å™¨é€Ÿåº¦ä¿¡æ¯"""
    try:
        client, _, _ = get_downloader_instance(downloader.id)
        if not client:
            return {
                'category': downloader.category,
                'name': f'{downloader.name} é“¾æ¥å¤±è´¥',
                'free_space_on_disk': 0,
                'connection_status': False,
                'dl_info_data': 0,
                'dl_info_speed': 0,
                'up_info_data': 0,
                'up_info_speed': 0,
            }
        if downloader.category == DownloaderCategory.qBittorrent:
            # x = {'connection_status': 'connected', 'dht_nodes': 0, 'dl_info_data': 2577571007646,
            #      'dl_info_speed': 3447895, 'dl_rate_limit': 41943040, 'up_info_data': 307134686158,
            #      'up_info_speed': 4208516, 'up_rate_limit': 0, 'category': 'Qb', 'name': 'home-qb'}
            info = client.sync_maindata().get('server_state')
            info.update({
                'category': downloader.category,
                'name': downloader.name,
                'connection_status': True if info.get('connection_status') == 'connected' else False,
            })
            return info
        elif downloader.category == DownloaderCategory.Transmission:
            base_info = client.session_stats().fields
            return {
                'connection_status': True,
                'free_space_on_disk': client.raw_session.get('download-dir-free-space'),
                # 'dht_nodes': 0,
                'dl_info_data': base_info.get('cumulative-stats').get('downloadedBytes'),
                'dl_info_speed': base_info.get('downloadSpeed'),
                # 'dl_rate_limit': 41943040,
                'up_info_data': base_info.get('cumulative-stats').get('uploadedBytes'),
                'up_info_speed': base_info.get('uploadSpeed'),
                # 'up_rate_limit': 0,
                'category': downloader.category,
                'name': downloader.name
            }
        else:
            return {
                'category': downloader.category,
                'name': downloader.name,
                'connection_status': False,
                'free_space_on_disk': 0,
                'dl_info_data': 0,
                'dl_info_speed': 0,
                'up_info_data': 0,
                'up_info_speed': 0,
            }
    except Exception as e:
        return {
            'category': downloader.category,
            'name': downloader.name,
            'free_space_on_disk': 0,
            'connection_status': False,
            'dl_info_data': 0,
            'dl_info_speed': 0,
            'up_info_data': 0,
            'up_info_speed': 0,
        }


def push_torrents_to_downloader(
        client,
        downloader_category: DownloaderCategory.choices,
        urls: Union[List[str], str],
        category: str = None,
        save_path: str = None,
        cookie: str = '',
        upload_limit: int = 0,
        download_limit: int = 150,
        is_paused: bool = None,
        is_skip_checking: bool = None,
        use_auto_torrent_management: bool = None,
):
    """å°†è¾…ç§æ•°æ®æ¨é€è‡³ä¸‹è½½å™¨"""
    # æš‚åœæ¨¡å¼æ¨é€è‡³ä¸‹è½½å™¨ï¼ˆåŒ…å«å‚æ•°ï¼Œä¸‹è½½é“¾æ¥ï¼ŒCookieï¼Œåˆ†ç±»æˆ–è€…ä¸‹è½½è·¯å¾„ï¼‰
    # å¼€å§‹æ ¡éªŒ
    # éªŒè¯æ ¡éªŒç»“æœï¼Œä¸ä¸ºç™¾åˆ†ç™¾çš„ï¼Œæš‚åœä»»åŠ¡
    if downloader_category == DownloaderCategory.qBittorrent:
        res = client.torrents.add(
            urls=urls.split('|'),
            category=category,
            save_path=save_path,
            is_skip_checking=is_skip_checking,
            is_paused=is_paused,
            upload_limit=upload_limit * 1024 * 1024,
            download_limit=download_limit * 1024 * 1024,
            use_auto_torrent_management=use_auto_torrent_management,
            cookie=cookie,
        )
        if res == 'Ok.':
            return CommonResponse.success(msg=f'ç§å­å·²æ·»åŠ ï¼Œè¯·æ£€æŸ¥ä¸‹è½½å™¨ï¼{res}')
        return CommonResponse.error(msg=f'ç§å­æ·»åŠ å¤±è´¥ï¼{res}')
    if downloader_category == DownloaderCategory.Transmission:
        msg_list = []
        for url in urls.split('|'):
            try:
                res = client.add_torrent(
                    torrent=url,
                    download_dir=category,
                    paused=is_paused,
                    cookies=cookie
                )
                if res.hashString and len(res.hashString) >= 0:
                    msg = f'ç§å­å·²æ·»åŠ ï¼Œè¯·æ£€æŸ¥ä¸‹è½½å™¨ï¼{res.name}'
                    logger.info(msg)
                else:
                    msg = f'ç§å­æ·»åŠ å¤±è´¥ï¼š{url}'
                    logger.warning(msg)
            except Exception as e:
                msg = f'ç§å­æ·»åŠ å¤±è´¥ï¼š{url}  {e}'
                logger.error(msg)
            msg_list.append(msg)
        return CommonResponse.success(msg='ï¼Œ'.join(msg_list))


def package_files(
        client, hash_string, package_size: int = 10,
        delete_one_file: bool = False,
        package_percent: float = 0.1
):
    """
    ç§å­æ–‡ä»¶æ‹†åŒ…ï¼Œåªä¸‹è½½éƒ¨åˆ†ï¼Œé»˜è®¤å¤§äº10Gçš„ç§å­æ‰è¿›è¡Œæ‹†åŒ…
    :param package_percent: æ‹†åŒ…åˆ°å¤šå°,åŸå¤§å°çš„ååˆ†ä¹‹ä¸€
    :param delete_one_file: åªæœ‰ä¸€ä¸ªæ–‡ä»¶ä¸”è¾¾åˆ°æ‹†åŒ…æ ‡å‡†æ—¶æ˜¯å¦åˆ é™¤
    :param package_size: æ‹†åŒ…å¤§å°ï¼Œå•ä½GB
    :param client: ä¸‹è½½å™¨
    :param hash_string: HASH
    :return:
    """
    # ç§å­å±æ€§
    try:
        prop = client.torrents_properties(torrent_hash=hash_string)
        # ç§å­æ€»å¤§å°
        total_size = prop.get('total_size')
        # å¦‚æœæ–‡ä»¶æ€»å¤§å°å¤§äºpackage_sizeï¼Œåˆ™è¿›è¡Œæ‹†åŒ…ï¼Œæ•°å­—è‡ªå®šä¹‰
        if total_size <= package_size * 1024 * 1024 * 1024:
            client.torrents_resume(torrent_hashes=hash_string)
        if total_size > package_size * 1024 * 1024 * 1024:
            # è·å–ç§å­æ–‡ä»¶åˆ—è¡¨ä¿¡æ¯
            files = client.torrents_files(torrent_hash=hash_string)
            # è·å–æ‰€æœ‰æ–‡ä»¶index
            total_ids = [file.get('index') for file in files if file.get('priority') == 1]
            # ä»å¤§åˆ°å°æ’åˆ—ç§å­
            files = sorted(files, key=lambda x: x.get('size'), reverse=True)
            # åªæœ‰ä¸€ä¸ªæ–‡ä»¶ä¸”å¤§äº15Gçš„åˆ æ‰
            if len(files) == 1 and total_size > 15 * 1024 * 1024 * 1024 and delete_one_file:
                client.torrents_delete(torrent_hash=hash_string)
                return
            # ä¸¤ä¸ªæ–‡ä»¶çš„
            if len(files) == 2:
                # å¦‚æœç¬¬äºŒä¸ªæ–‡ä»¶å¤§å°å°äº500Mæˆ–è€…å¤§äº15Gçš„åˆ æ‰
                # if files[1].size < 500 * 1024 * 1024 or files[1].size > 15 * 1024 * 1024 * 1024:
                #     client.torrents_delete(torrent_hash=hash_string)
                # è®¾ç½®åªä¸‹è½½ç¬¬äºŒä¸ªæ–‡ä»¶
                client.torrents_file_priority(
                    torrent_hash=hash_string,
                    file_ids=0,
                    priority=0
                )
                return
            # è¶…è¿‡ä¸‰ä¸ªæ–‡ä»¶çš„ï¼Œå…ˆæ’é™¤æœ€å¤§çš„å’Œæœ€å°çš„
            files = files[1:-1]
            # ç„¶åæ‰“ä¹±é¡ºåº
            random.shuffle(files)
            ids = []
            size = 0
            # å¾ªç¯è·å–æ–‡ä»¶indexï¼Œå½“æ€»å¤§å°è¶…è¿‡æ€»å¤§å°çš„ååˆ†ä¹‹ä¸€æ—¶ç»“æŸ
            for file in files:
                size += file.get('size')
                ids.append(file.get('index'))
                if size > total_size * package_percent:
                    break
            # å¦‚æœæœ€åè·å–çš„æ–‡ä»¶å¤§å°å°äº800M
            # if size < 500 * 1024 * 1024:
            #     client.torrents_delete(torrent_hash=hash_string)
            #     return
            # è®¡ç®—éœ€è¦å–æ¶ˆä¸‹è½½çš„æ–‡ä»¶indexåˆ—è¡¨ï¼Œå°†æ€»åˆ—è¡¨å’Œéœ€è¦ä¸‹è½½çš„åˆ—è¡¨è½¬ä¸ºé›†åˆåç›¸å‡
            delete_ids = list(set(total_ids) - set(ids))
            if len(delete_ids) > 0:
                logger.info(f'éœ€è¦å–æ¶ˆä¸‹è½½çš„æ–‡ä»¶IDï¼š{delete_ids}')
                client.torrents_file_priority(
                    torrent_hash=hash_string,
                    file_ids=delete_ids,
                    priority=0
                )
                client.torrents_resume(torrent_hash=hash_string)
                msg = f'ç§å­ {hash_string} æ‹†åŒ…å®Œæˆ'
                logger.info(msg)
            else:
                msg = f'ç§å­ {hash_string} æ— éœ€æ‹†åŒ…ï¼Œè·³è¿‡'
                logger.info(msg)
            return CommonResponse.success(msg=msg)
    except Exception as e:
        msg = f'ç§å­ {hash_string} æ‹†åŒ…å¤±è´¥ï¼'
        logger.error(f'{traceback.format_exc(3)} \n {msg}')
        return CommonResponse.error(msg=msg)


def filter_torrent_by_rules(mysite: MySite, torrents: List[TorrentInfo]):
    """
    ä½¿ç”¨ç«™ç‚¹é€‰ä¸­è§„åˆ™ç­›é€‰ç§å­
    :param mysite: æˆ‘çš„ç«™ç‚¹
    :param torrents: ç§å­åˆ—è¡¨
    :return: ç­›é€‰è¿‡åçš„ç§å­åˆ—è¡¨
    """
    rules = demjson3.decode(mysite.remove_torrent_rules).get('push')
    logger.info(f"å½“å‰ç«™ç‚¹ï¼š{mysite.nickname}, é€‰ç§è§„åˆ™ï¼š{rules}")

    # æ”¶é›†ä¸ç¬¦åˆè§„åˆ™çš„ç§å­
    excluded_torrents = []

    for torrent in torrents:
        try:
            logger.info(f'é€‰ç§å¼€å§‹ï¼Œå½“å‰ç§å­ï¼š{torrent.title}')
            # åŒ…å«å…³é”®å­—å‘½ä¸­
            includes = rules.get('include')
            if includes and len(includes) > 0:
                logger.debug(f'å½“å‰åŒ…å«å…³é”®å­—æ£€æŸ¥ï¼š{includes}')
                if not any(rule in torrent.title for rule in includes):
                    logger.info(f'å…³é”®å­—æœªå‘½ä¸­ï¼Œç»§ç»­')
                    excluded_torrents.append(torrent)
                    # è·³è¿‡è¯¥ç§å­çš„å¤„ç†ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªç§å­çš„åˆ¤æ–­
                else:
                    logger.info(f'å…³é”®å­—å‘½ä¸­ï¼')
                    continue

            # æ’é™¤å…³é”®å­—å‘½ä¸­
            excludes = rules.get('exclude')
            if excludes and len(excludes) > 0:
                logger.debug(f'å½“å‰æ’é™¤å…³é”®å­—æ£€æŸ¥ï¼š{includes}')

                if all(rule not in torrent.title for rule in excludes):
                    logger.info(f'æ’é™¤å…³é”®å­—æœªå‘½ä¸­ï¼Œè·³è¿‡')
                    excluded_torrents.append(torrent)
                    torrent.state = 5
                    torrent.save()
                    # è·³è¿‡è¯¥ç§å­çš„å¤„ç†ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªç§å­çš„åˆ¤æ–­
                    continue

            # ç§å­å¤§å°å‘½ä¸­
            size = rules.get('size')
            if size:
                min_size = size.get('min') * 1024 * 1024 * 1024
                max_size = size.get('max') * 1024 * 1024 * 1024
                logger.info(
                    f'å½“å‰ç§å­å¤§å°æ£€æµ‹ï¼šè®¾å®šæœ€å°ï¼š{size.get("min")} GB,'
                    f'è®¾å®šæœ€å¤§ï¼š{size.get("max")} GB,'
                    f'å½“å‰ï¼š{int(torrent.size) / 1024 / 1024 / 1024} GB'
                )
                if not int(min_size) < int(torrent.size) < int(max_size):
                    logger.warning('ğŸˆ² è§¦å‘ç§å­å¤§å°è§„åˆ™ï¼Œæ’é™¤')
                    excluded_torrents.append(torrent)
                    torrent.state = 5
                    torrent.save()
                    continue
            # å‰©ä½™å…è´¹æ—¶é—´å‘½ä¸­
            sale_expire = rules.get('sale_expire')
            if sale_expire:
                logger.debug(
                    f'è®¾å®šå‰©ä½™å…è´¹æ—¶é—´ï¼š{sale_expire}ï¼Œå½“å‰ç§å­å…è´¹åˆ°æœŸæ—¶é—´ï¼š{torrent.sale_status}: {torrent.sale_expire}')
                exp = torrent.sale_expire
                if not exp:
                    logger.warning(f'ğŸˆ² å½“å‰ç§å­ä¼˜æƒ åˆ°æœŸæ—¶é—´ï¼š{exp}, è·³è¿‡')
                else:
                    if isinstance(exp, str):
                        exp = datetime.strptime(exp, "%Y-%m-%d %H:%M:%S")
                    # å¦‚æœç§å­æœ‰åˆ°æœŸæ—¶é—´ï¼Œä¸”åˆ°æœŸæ—¶é—´å°äºè®¾å®šå€¼ï¼Œæ’é™¤
                    if isinstance(exp, datetime) and (exp - datetime.now()).total_seconds() < sale_expire:
                        logger.warning('ğŸˆ² è§¦å‘å‰©ä½™å…è´¹æ—¶é—´è§„åˆ™ï¼Œæ’é™¤')
                        excluded_torrents.append(torrent)
                        # è·³è¿‡è¯¥ç§å­çš„å¤„ç†ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªç§å­çš„åˆ¤æ–­
                        torrent.state = 5
                        torrent.save()
                        continue
            # å‘ç§æ—¶é—´å‘½ä¸­
            published = rules.get('published')
            if published:
                logger.info(f'å‘ç§æ—¶é—´æ£€æŸ¥ï¼š{published}')
                torrent_published = datetime.strptime(torrent.published, "%Y-%m-%d %H:%M:%S") if isinstance(
                    torrent.published, str) else torrent.published
                logger.info(f'å½“å‰ç§å­å‘ç§æ—¶é—´æ£€æŸ¥ï¼š{torrent_published}')
                if (datetime.now() - torrent_published).total_seconds() > published:
                    logger.warning('ğŸˆ² è§¦å‘å‘ç§æ—¶é—´è§„åˆ™ï¼Œæ’é™¤')
                    excluded_torrents.append(torrent)
                    # è·³è¿‡è¯¥ç§å­çš„å¤„ç†ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªç§å­çš„åˆ¤æ–­
                    torrent.state = 5
                    torrent.save()
                    continue
            # åšç§äººæ•°å‘½ä¸­
            seeders = rules.get('seeders')
            if seeders:
                logger.debug(f'è®¾å®šåšç§äººæ•°ï¼š{seeders}ï¼Œå½“å‰ç§å­åšç§äººæ•°ï¼š{torrent.seeders}')
                if torrent.seeders > seeders:
                    logger.warning('ğŸˆ² è§¦å‘å½“å‰åšç§äººæ•°è§„åˆ™ï¼Œæ’é™¤')
                    excluded_torrents.append(torrent)
                    # è·³è¿‡è¯¥ç§å­çš„å¤„ç†ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªç§å­çš„åˆ¤æ–­
                    continue
            # ä¸‹è½½äººæ•°å‘½ä¸­
            leechers = rules.get('leechers')
            if leechers:
                logger.debug(f'è®¾å®šä¸‹è½½äººæ•°ï¼š{leechers}ï¼Œå½“å‰ç§å­ä¸‹è½½äººæ•°ï¼š{torrent.leechers}')
                if torrent.leechers < leechers:
                    logger.warning('ğŸˆ² è§¦å‘å½“å‰ä¸‹è½½äººæ•°è§„åˆ™ï¼Œæ’é™¤')
                    excluded_torrents.append(torrent)
                    # è·³è¿‡è¯¥ç§å­çš„å¤„ç†ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªç§å­çš„åˆ¤æ–­
                    continue

        except Exception:
            logger.error(traceback.format_exc(3))
            continue

    # ä¸€æ¬¡æ€§ä¿å­˜æ‰€æœ‰ä¸ç¬¦åˆè§„åˆ™çš„ç§å­
    # for excluded_torrent in excluded_torrents:
    #     excluded_torrent.state = 5
    #     excluded_torrent.save()

    # è¿”å›ç¬¦åˆè§„åˆ™çš„ç§å­åˆ—è¡¨
    return list(set(torrents) - set(excluded_torrents))


def sha1_hash(string: str) -> str:
    return hashlib.sha1(string.encode()).hexdigest()


def remove_torrent_by_site_rules(mysite: MySite):
    """
    ç«™ç‚¹åˆ ç§
    :param mysite:
    :return msg
    """
    rules = demjson3.decode(mysite.remove_torrent_rules).get('remove')
    logger.info(f"å½“å‰ç«™ç‚¹ï¼š{mysite}, åˆ ç§è§„åˆ™ï¼š{rules}")
    logger.info(f"å½“å‰ä¸‹è½½å™¨ï¼š{mysite.downloader_id}")
    client, _, _ = get_downloader_instance(mysite.downloader.id)
    if not client:
        return CommonResponse.error(msg=f'{mysite.nickname} - ä¸‹è½½å™¨ {mysite.downloader.name} é“¾æ¥å¤±è´¥!')
    website = WebSite.objects.get(id=mysite.site)
    count = 0
    hashes = []
    expire_hashes = []
    torrents = [torrent for torrent in client.torrents_info() if torrent.get('category').find(website.nickname) >= 0]
    logger.info(f'å½“å‰ä¸‹è½½å™¨å…±æœ‰ç«™ç‚¹ {website.name} ç§å­æ•°é‡ï¼š{len(torrents)}')
    # hash_torrents = {item.get('hash'): item for item in torrents}
    logger.info(f'å¼€å§‹å¾ªç¯å¤„ç†ç§å­')
    torrent_infos = mysite.torrentinfo_set.all()
    for torrent in torrents:
        category = torrent.get('category')
        hash_string = torrent.get('hash')
        if category.find('-'):
            try:
                _, tid = category.split('-')
                logger.info(f'å½“å‰ç§å­IDï¼š{tid}')
                torrent_info = torrent_infos.get(tid=tid)
                torrent_info.hash_string = hash_string
            except my_site.models.TorrentInfo.DoesNotExist:
                logger.info(f'æœªåœ¨æ•°æ®åº“ä¸­æ‰¾åˆ°å½“å‰ç§å­')
                continue
            except Exception as e:
                logger.error(f'æŸ¥æ‰¾å½“å‰ç§å­å¤±è´¥ï¼š{e}')
                logger.error(traceback.format_exc(5))
                continue
        else:
            logger.warning(f'éæœ¬å·¥å…·åˆ·æµç§å­ï¼Œè·³è¿‡ï¼')
            continue

        try:
            # é€šè¿‡qbittorrentapiå®¢æˆ·ç«¯è·å–ç§å­çš„å—å“ˆå¸Œåˆ—è¡¨å’Œæ–‡ä»¶åˆ—è¡¨ï¼Œå¹¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            logger.info(f'å¼€å§‹å®Œå–„ç§å­ä¿¡æ¯')
            try:
                if not torrent_info.pieces_qb:
                    # è·å–ç§å­å—HASHåˆ—è¡¨ï¼Œå¹¶ç”Ÿæˆç§å­å—HASHåˆ—è¡¨å­—ç¬¦ä¸²çš„sha1å€¼ï¼Œä¿å­˜
                    pieces_hash_list = client.torrents_piece_hashes(torrent_hash=hash_string)
                    pieces_hash_string = ''.join(str(pieces_hash) for pieces_hash in pieces_hash_list)
                    torrent_info.pieces_qb = sha1_hash(pieces_hash_string)
                if not torrent_info.filelist:
                    # è·å–æ–‡ä»¶åˆ—è¡¨ï¼Œå¹¶ç”Ÿæˆæ–‡ä»¶åˆ—è¡¨å­—ç¬¦ä¸²çš„sha1å€¼ï¼Œä¿å­˜
                    file_list = client.torrents_files(torrent_hash=hash_string)
                    file_list_hash_string = ''.join(str(item) for item in file_list)
                    torrent_info.filelist = sha1_hash(file_list_hash_string)
                    torrent_info.files_count = len(file_list)
                torrent_info.save()
            except qbittorrentapi.exceptions.NotFound404Error:
                msg = f'{torrent_info.title}: å®Œå–„hashå¤±è´¥!--ä¸‹è½½å™¨å·²åˆ ç§ï¼'
                torrent_info.state = 3
                torrent_info.save()
                logger.error(msg)
                continue
            except Exception as e:
                msg = f'{torrent_info.title}: å®Œå–„ç§å­å¤±è´¥ï¼'
                logger.error(traceback.format_exc(3))
                logger.error(msg)
            # ä¿å­˜ç§å­å±æ€§
            torrent_info.save()

            # åˆ ç§
            logger.info(f'{torrent_info.title} - å¼€å§‹åŒ¹é…åˆ ç§è§„åˆ™: {hash_string}')
            prop = client.torrents_properties(torrent_hash=hash_string)

            # ç£ç›˜ç©ºé—´æ£€æŸ¥
            keep_free_space = rules.get('keep_free_space')
            if keep_free_space and keep_free_space > 0:
                logger.debug(f'è®¾å®šä¸åˆ ç§ç©ºé—´å¤§å°ï¼š{keep_free_space} GB')
                free_space = client.sync_maindata().get('server_state').get('free_space_on_disk')
                logger.debug(f'å½“å‰ä¸‹è½½å™¨å‰©ä½™ç©ºé—´å¤§å°ï¼š{free_space / 1024 / 1024 / 1024} GB ')
                if free_space > keep_free_space * 1024 * 1024 * 1024:
                    logger.info(f'ä¸‹è½½å™¨å‰©ä½™ç©ºé—´å……è¶³ï¼Œä¸æ‰§è¡Œåˆ ç§æ“ä½œï¼')
                    continue

            # æ’é™¤å…³é”®å­—å‘½ä¸­ï¼Œ
            delete_flag = False
            logger.info(f'æ’é™¤å…³é”®å­—: {hash_string}')
            if rules.get('exclude'):
                for rule in rules.get('exclude'):
                    if torrent.get('title').find(rule) > 0:
                        delete_flag = True
                        logger.info(f"ğŸš«{mysite.nickname} {torrent.get('tid')} æ’é™¤å…³é”®å­—å‘½ä¸­ï¼š{delete_flag}")
                        break
            if delete_flag:
                # é‡åˆ°è¦æ’é™¤çš„å…³é”®å­—çš„ç§å­ï¼Œç›´æ¥è·³è¿‡ï¼Œä¸å†ç»§ç»­æ‰§è¡Œåˆ ç§
                continue

            # å…è´¹åˆ°æœŸæ£€æµ‹
            logger.info(f'å…è´¹åˆ°æœŸæ£€æµ‹: {hash_string}')
            if not torrent_info.sale_expire:
                logger.info(f'{torrent_info.title} å…è´¹è¿‡æœŸæ—¶é—´ï¼š{torrent_info.sale_expire}')
            else:
                torrent_sale_expire = torrent_info.sale_expire.timestamp()
                sale_expire = rules.get('sale_expire', {"expire": 300, "delete_on_completed": True})
                expire_time = sale_expire.get('expire', 300)
                delete_flag = sale_expire.get('delete_on_completed')
                if torrent_sale_expire - time.time() <= expire_time and (prop.get('completion_date') < 0 or (
                        prop.get('completion_date') > 0 and delete_flag)):
                    expire_hashes.append(hash_string)
                    logger.debug(f'ğŸš«{torrent_info.title} å…è´¹å³å°†åˆ°æœŸ å‘½ä¸­')
                    continue
            # æŒ‡å®šæ—¶é—´æ®µå†…å¹³å‡é€Ÿåº¦
            logger.info(f'æŒ‡å®šæ—¶é—´æ®µå†…å¹³å‡é€Ÿåº¦: {hash_string}')
            upload_speed_avg = rules.get("upload_speed_avg")
            logger.debug(f'æŒ‡å®šæ—¶é—´æ®µå†…å¹³å‡é€Ÿåº¦æ£€æµ‹: {upload_speed_avg}')
            if upload_speed_avg:
                # ä»ç¼“å­˜è·å–æŒ‡å®šæ—¶é—´æ®µå¹³å‡é€Ÿåº¦æ£€æµ‹æ•°æ®
                upload_speed_avg_list = cache.get(f'{hash_string}__update_uploaded', [])
                # å¦‚æœæ•°æ®ä¸ºç©ºï¼Œåˆ™æ·»åŠ åˆå§‹åŒ–æ•°æ®
                if len(upload_speed_avg_list) <= 0:
                    cache.set(f'{hash_string}_update_uploaded', [{
                        "check_time": prop.get("addition_date"),
                        "uploaded": 0
                    }])
                # è·å–å½“å‰æ•°æ®å¹¶æ·»åŠ åˆ°æ£€æµ‹æ•°æ®ä¸­
                now = time.time()
                now_uploaded = prop.get("total_uploaded")
                upload_speed_avg_list.append({
                    "check_time": now,
                    "uploaded": now_uploaded
                })
                # è·å–åˆ—è¡¨ä¸­æœ€æ—©çš„ä¸€æ¡æ•°æ®
                earliest_uploaded_info = upload_speed_avg_list[0]
                # è®¡ç®—å½“å‰æ•°æ®ä¸æœ€æ—©ä¸€æ¡æ•°æ®çš„æ—¶é—´å·®
                time_delta = now - earliest_uploaded_info.get("check_time")
                if time_delta < upload_speed_avg.get("time"):
                    # è‹¥æ—¶é—´å·®æœªè¾¾åˆ°æ£€æµ‹æ—¶é—´ï¼Œè·³è¿‡
                    logger.info(f'å½“å‰ç§å­è¿è¡Œæ—¶é—´æœªè¾¾åˆ°æ£€æµ‹æ—¶é—´ï¼š{time_delta} < {upload_speed_avg.get("time")}')
                else:
                    # è¾¾åˆ°æ£€æµ‹æ ‡å‡†çš„ï¼Œè·å–æ•°æ®çš„ä¸Šä¼ é‡ä¿¡æ¯ï¼Œè®¡ç®—è¿™æ—¶é—´æ®µå†…çš„å¹³å‡é€Ÿåº¦
                    earliest_uploaded = earliest_uploaded_info.get("uploaded")
                    uploaded_eta = now_uploaded - earliest_uploaded
                    uploaded_avg = uploaded_eta / time_delta
                    logger.debug(f'{torrent_info.title} ä¸Šä¼ é€Ÿåº¦ {uploaded_avg / 1024} ')
                    if uploaded_avg < upload_speed_avg.get("upload_speed") * 1024:
                        # å¦‚æœå¹³å‡é€Ÿåº¦ä¸è¾¾æ ‡ï¼Œåˆ ç§
                        cache.remove(f'{hash_string}_update_uploaded')
                        logger.debug(f'ğŸš« {upload_speed_avg.get("upload_speed")} ä¸è¾¾æ ‡åˆ ç§ å‘½ä¸­')
                        hashes.append(hash_string)
                        continue
                    else:
                        # è¾¾æ ‡çš„ï¼Œåˆ é™¤æœ€æ—©çš„æ•°æ®ï¼Œå¹¶å°†æ–°æ•°æ®å­˜å…¥ç¼“å­˜
                        upload_speed_avg_list.pop(0)
                        cache.set(f'{hash_string}_update_uploaded', upload_speed_avg_list)
            logger.debug(f'{hash_string} -- {torrent_info.title} ä¸Šä¼ é€Ÿåº¦ä¸è¾¾æ ‡åˆ ç§ æœªå‘½ä¸­')
            logger.debug(f'ç«™ç‚¹åˆ ç§æ£€æµ‹')
            not_registered_msg = [
                'torrent not registered with this tracker',
                'err torrent deleted due to other',
                'Torrent not exists',
            ]
            trackers = client.torrents_trackers(torrent_hash=hash_string)
            tracker_checked = False
            for tracker in trackers:
                delete_msg = [msg for msg in not_registered_msg if tracker.get('msg').lower().startswith(msg)]
                if len(delete_msg) > 0:
                    hashes.append(hash_string)
                    tracker_checked = True
                    break
            if tracker_checked:
                logger.debug(f'ğŸš«{hash_string} -- {torrent_info.title} ç«™ç‚¹åˆ ç§ å‘½ä¸­')
                torrent_info.state = 4
                torrent_info.save()
                continue
            else:
                logger.debug(f'{hash_string} -- {torrent_info.title} ç«™ç‚¹åˆ ç§ æœªå‘½ä¸­')
            # å®Œæˆäººæ•°è¶…æ ‡åˆ é™¤
            logger.info(f'å®Œæˆäººæ•°æ£€æµ‹: {torrent_info.title}')
            torrent_num_complete = rules.get("num_completer")
            logger.debug(f'{hash_string} -- {torrent_info.title} å®Œæˆäººæ•°è¶…æ ‡åˆ é™¤: {torrent_num_complete}')
            if torrent_num_complete:
                completers = torrent_num_complete.get("completers")
                upspeed = torrent_num_complete.get("upspeed")
                num_complete = prop.get('seeds_total')
                logger.debug(f'å½“å‰ç§å­å·²å®Œæˆäººæ•°ï¼š{num_complete} -- è®¾å®šäººæ•°ï¼š{completers}')
                logger.debug(f'å½“å‰ç§å­ä¸Šä¼ é€Ÿåº¦ï¼š{torrent.get("upspeed")} -- è®¾å®šé€Ÿåº¦ï¼š{upspeed}')
                if num_complete > completers and torrent.get("upspeed") < upspeed:
                    logger.debug(
                        f'ğŸš«{torrent_info.title} å®Œæˆäººæ•° {num_complete} è¶…æ ‡,'
                        f'å½“å‰é€Ÿåº¦ {torrent.get("upspeed")} ä¸è¾¾æ ‡ å‘½ä¸­'
                    )
                    hashes.append(hash_string)
                    continue
                logger.debug(f'{hash_string} -- {torrent_info.title} å®Œæˆäººæ•°æœªè¶…æ ‡ æœªå‘½ä¸­')
            # æ­£åœ¨ä¸‹è½½äººæ•° ä½äºè®¾å®šå€¼åˆ é™¤
            logger.info(f'æ­£åœ¨ä¸‹è½½äººæ•°æ£€æµ‹: {torrent_info.title}')
            torrent_num_incomplete = rules.get("num_incomplete")
            logger.debug(f'å®Œæˆäººæ•°è¶…æ ‡åˆ é™¤: {torrent_num_incomplete}')
            if torrent_num_incomplete and len(torrent_num_incomplete) > 0:
                num_incomplete = torrent.get('num_incomplete')
                logger.debug(f'{hash_string} -- {torrent_info.title} æ­£åœ¨ä¸‹è½½å®Œæˆäººæ•°ä¸è¾¾æ ‡: {num_incomplete}')
                if num_incomplete < torrent_num_incomplete:
                    logger.debug(f'ğŸš«{hash_string} -- {torrent_info.title} æ­£åœ¨ä¸‹è½½äººæ•° ä½äºè®¾å®šå€¼ å‘½ä¸­')
                    hashes.append(hash_string)
                    continue
                logger.debug(f'{hash_string} -- {torrent_info.title} æ­£åœ¨ä¸‹è½½äººæ•° é«˜äºè®¾å®šå€¼ æœªå‘½ä¸­')
            # æ— ä¸Šä¼ æ— ä¸‹è½½è¶…æ—¶åˆ ç§
            logger.info(f'æ´»è·ƒåº¦æ£€æµ‹: {hash_string}')
            logger.debug(f'å®Œæˆäººæ•°è¶…æ ‡åˆ é™¤: {rules.get("timeout") and rules.get("timeout") > 0}')
            if rules.get("timeout") and rules.get("timeout") > 0:
                last_activity = torrent.get('last_activity')
                logger.debug(f'{hash_string} -- {torrent_info.title} æœ€åæ´»åŠ¨æ—¶é—´: {time.time() - last_activity}')
                if time.time() - last_activity > rules.get("timeout"):
                    logger.debug(f'ğŸš«{hash_string} -- {torrent_info.title} æ— æ´»åŠ¨è¶…æ—¶ å‘½ä¸­')
                    hashes.append(hash_string)
                    continue
                logger.debug(f'{hash_string} -- {torrent_info.title} æ— æ´»åŠ¨è¶…æ—¶ æœªå‘½ä¸­')
            # è¿›åº¦ä¸å¹³å‡ä¸Šä¼ é€Ÿåº¦è¾¾æ ‡æ£€æµ‹
            progress = torrent.get('progress')
            progress_check = rules.get("progress_check")
            logger.debug(f'è¿›åº¦ä¸å¹³å‡ä¸Šä¼ é€Ÿåº¦è¾¾æ ‡æ£€æµ‹: {progress_check}')
            if progress_check and len(progress_check) > 0:
                progress_checked = False
                for key, value in progress_check.items():
                    logger.debug(
                        f'{hash_string}-{torrent_info.title} æŒ‡å®šè¿›åº¦{float(key)},'
                        f'æŒ‡å®šé€Ÿåº¦{value / 1024} MB/S,å¹³å‡ä¸Šä¼ é€Ÿåº¦: {prop.get("up_speed_avg") / 1024 / 1024} MB/S'
                    )
                    if progress < float(key):
                        continue
                    elif prop.get('up_speed_avg') < value * 1024:
                        hashes.append(hash_string)
                        progress_checked = True
                        logger.debug(
                            f'ğŸš«{hash_string}-{torrent_info.title} æŒ‡å®šè¿›åº¦ä¸å¹³å‡ä¸Šä¼ é€Ÿåº¦è¾¾æ ‡æ£€æµ‹ä½äºè®¾å®šå€¼ å‘½ä¸­')
                        break
                if progress_checked:
                    continue
                logger.debug(f'{hash_string} -- {torrent_info.title} æŒ‡å®šè¿›åº¦ä¸å¹³å‡ä¸Šä¼ é€Ÿåº¦è¾¾æ ‡æ£€æµ‹ é«˜äºè®¾å®šå€¼ æœªå‘½ä¸­')
            # è¾¾åˆ°æŒ‡å®šåˆ†äº«ç‡
            ratio = prop.get('share_ratio')

            if rules.get("max_ratio"):
                logger.info(f'åˆ†äº«ç‡æ£€æµ‹: {hash_string}')
                if progress < 1:
                    logger.debug(f'{hash_string} -- {torrent_info.title} å°šæœªä¸‹è½½å®Œæ¯•ï¼ æœªå‘½ä¸­')
                elif ratio >= rules.get("max_ratio"):
                    hashes.append(hash_string)
                    logger.debug(f'ğŸš«{hash_string} -- {torrent_info.title} å·²è¾¾åˆ°æŒ‡å®šåˆ†äº«ç‡ å‘½ä¸­')
                    continue
                else:
                    logger.debug(f'{hash_string} -- {torrent_info.title} æœªè¾¾åˆ°æŒ‡å®šåˆ†äº«ç‡ æœªå‘½ä¸­')

            # æŒ‡å®šæ—¶é—´æ®µå†…åˆ†äº«ç‡ä¸è¾¾æ ‡
            ratio_check = rules.get("ratio_check")
            logger.debug(f'æŒ‡å®šæ—¶é—´æ®µå†…åˆ†äº«ç‡ä¸è¾¾æ ‡: {ratio_check}')
            if ratio_check and len(ratio_check) > 0:
                ratio_checked = False
                time_active = prop.get('time_elapsed')
                for key, value in ratio_check.items():
                    logger.debug(
                        f'{hash_string}-{torrent_info.title} æŒ‡å®šæ—¶é•¿{float(key)},æŒ‡å®šåˆ†äº«ç‡{value}ï¼Œå½“å‰åˆ†äº«ç‡ï¼š{ratio}'
                    )
                    if time_active < float(key):
                        logger.debug(f'æ´»åŠ¨æ—¶é—´ï¼š{time_active / 60}åˆ†é’Ÿ å°šæœªè¾¾åˆ°æŒ‡å®šä¸‹è½½æ—¶é•¿ï¼š{float(key)}')
                        continue
                    if ratio < value:
                        logger.debug(f'ğŸš«{hash_string} -- {torrent_info.title} æŒ‡å®šæ—¶é—´æ®µå†…åˆ†äº«ç‡ä¸è¾¾æ ‡ ä½äºè®¾å®šå€¼ å‘½ä¸­')
                        hashes.append(hash_string)
                        ratio_checked = True
                        break
                if ratio_checked:
                    continue
                logger.debug(f'{hash_string} -- {torrent_info.title} æŒ‡å®šæ—¶é—´æ®µå†…åˆ†äº«ç‡è¾¾æ ‡ æœªå‘½ä¸­')
        except qbittorrentapi.exceptions.NotFound404Error:
            msg = f'{torrent_info.title}: å®Œå–„ä¿¡æ¯å¤±è´¥!--ä¸‹è½½å™¨å·²åˆ ç§ï¼'
            torrent_info.state = 3
            logger.error(msg)
            torrent_info.save()
        except Exception as e:
            logger.error(traceback.format_exc(3))
            msg = 'å®Œå–„ç§å­æˆ–è§£æåˆ ç§è§„åˆ™å¤±è´¥ï¼'
            logger.error(msg)
            continue
    logger.info(
        f'ğŸš«{mysite.nickname}-æœ¬æ¬¡è¿è¡Œå®Œå–„{count}ä¸ªç§å­ä¿¡æ¯ï¼åˆ ç§è§„åˆ™å‘½ä¸­ä»»åŠ¡:{len(hashes)}ä¸ªï¼Œå…è´¹å³å°†åˆ°æœŸå‘½ä¸­ï¼š{len(expire_hashes)}ä¸ª')
    try:
        count = 0
        if len(hashes) + len(expire_hashes) > 0:
            client.torrents_reannounce(torrent_hashes=hashes)
            # å•æ¬¡æœ€å¤šåˆ ç§æ•°é‡, ä¸å¡«å†™é»˜è®¤æ‰€æœ‰è¢«ç­›é€‰çš„, å…è´¹åˆ°æœŸçš„ä¸ç®—åœ¨å†…
            num_delete = rules.get("num_delete", 0)
            if num_delete > 0:
                random.shuffle(hashes)
                hashes = hashes[:num_delete]
            hashes.extend(expire_hashes)
            client.torrents_delete(torrent_hashes=hashes, delete_files=True)
            # å¯¹å·²åˆ é™¤çš„ç§å­ä¿¡æ¯è¿›è¡Œå½’æ¡£
            count = TorrentInfo.objects.filter(
                hash_string__in=hashes
            ).update(state=5, downloader=None)
            msg = f'{mysite.nickname}ï¼šæœ¬æ¬¡è¿è¡Œåˆ é™¤ç§å­{count}ä¸ªï¼'
        else:
            msg = f'{mysite.nickname}ï¼šæœ¬æ¬¡è¿è¡Œæ²¡æœ‰ç§å­è¦åˆ é™¤ï¼'
        logger.info(msg)
        # å¯åŠ¨æ‰€æœ‰ç§å­
        client.torrents_resume(torrent_hashes='all')
        return CommonResponse.success(msg=msg, data=count)
    except Exception as e:
        logger.error(traceback.format_exc(3))
        return CommonResponse.error(msg=f'{mysite.nickname} - åˆ ç§å‡ºé”™å•¦ï¼')


def torrents_filter_by_percent_completed_rule(client, num_complete_percent, downloaded_percent):
    """
    ç§å­ç­›é€‰ä¹‹ ä¸‹è½½è¿›åº¦ç­›é€‰
    :param client: å®¢æˆ·ç«¯ï¼Œä»…æ”¯æŒQB
    :param num_complete_percent: è¾¾æ ‡äººæ•°
    :param downloaded_percent: å·²å®Œæˆç™¾åˆ†æ¯”
    :return:
    """
    torrents = client.torrents.info()
    hashes = []
    for torrent in torrents:
        hash_string = torrent.get('hash')
        progress = torrent.get('progress')
        if progress >= 1:
            continue
        not_registered_msg = [
            'torrent not registered with this tracker',
            'err torrent deleted due to other',
            'err torrent deleted due to repack, related torrent: /details.php?id='
        ]
        trackers = client.torrents_trackers(torrent_hash=hash_string)
        tracker_checked = False
        for tracker in [t for t in trackers if t.get('tier') == 0]:
            delete_msg = [msg for msg in not_registered_msg if msg in tracker.get('msg')]
            if len(delete_msg) > 0:
                hashes.append(hash_string)
                tracker_checked = True
                break
        if tracker_checked:
            continue
        # if torrent.get('num_complete') > 10:
        #     hashes.append(hash_string)
        #     continue

        category = torrent.get('category')
        if len(category) <= 0:
            continue
        num_complete = torrent.get('num_complete')
        uploaded = torrent.get('uploaded')
        ratio = torrent.get('ratio')
        time_active = torrent.get('time_active')
        if time_active > 1800 and ratio < 0.01:
            hashes.append(hash_string)
            continue
        if num_complete > 5:
            hashes.append(hash_string)
            continue
        # elif time_active > 600 and uploaded / time_active < 50:
        #     hashes.append(hash_string)
        peer_info = client.sync_torrent_peers(torrent_hash=hash_string)
        peers = peer_info.get('peers').values()
        num_peers = len(peers)
        if num_peers > 0:
            progress = [peer.get('progress') for peer in peers]
            high_progress = [p for p in progress if p > downloaded_percent]
            if len(high_progress) / num_peers > num_complete_percent:
                hashes.append(hash_string)
    return hashes


def get_torrents_hash_from_iyuu(hash_list: List[str]):
    try:
        hash_list.sort()
        iyuu_token = parse_toml('repeat').get('iyuu_token')
        logger.info(f'IYUU - TOKENï¼š{iyuu_token}')
        # ç”±äºjsonè§£æçš„åŸå› ï¼Œåˆ—è¡¨å…ƒç´ ä¹‹é—´æœ‰ç©ºæ ¼ï¼Œéœ€è¦æ›¿æ¢æ‰æ‰€æœ‰ç©ºæ ¼
        hash_list_json = json.dumps(hash_list).replace(' ', '')
        hash_list_sha1 = hashlib.sha1(hash_list_json.encode(encoding='utf-8')).hexdigest()
        url = f'{os.getenv("IYUU_SERVER")}/index.php?s=App.Api.Infohash'
        data = {
            # IYUU token
            'sign': iyuu_token,
            # å½“å‰æ—¶é—´æˆ³
            'timestamp': int(time.time()),
            # å®¢æˆ·ç«¯ç‰ˆæœ¬
            'version': '2.0.1',
            # hashåˆ—è¡¨
            'hash': hash_list_json,
            # hashåˆ—è¡¨sha1
            'sha1': hash_list_sha1
        }
        res = requests.post(url=url, data=data).json()
        ret = res.get('ret')
        logger.info(f'è¾…ç§è¿”å›æ¶ˆæ¯ç ï¼š{ret}ï¼Œè¿”å›æ¶ˆæ¯ï¼š{res.get("msg")}ï¼Œè¿”å›æ•°æ®ï¼š{res.get("data")}')
        if ret == 200:
            site_list = WebSite.objects.all()
            iyuu_data = res.get('data')
            repeat_info = {}
            for hash_string, values in iyuu_data.items():
                try:
                    current_list = []

                    for t in values.get("torrent"):
                        info_hash = t.get('info_hash')
                        if info_hash in hash_list:
                            logger.warning(f'{info_hash} æœ¬åœ°å·²å­˜åœ¨ï¼Œè·³è¿‡ï¼')
                            continue
                        tid = t.get('torrent_id')
                        sid = t.get('sid')
                        try:
                            site = site_list.filter(iyuu=sid).first()
                            logger.info(f'å½“å‰ç«™ç‚¹ï¼š{site}')
                            if not isinstance(site, WebSite):
                                # ç«™ç‚¹å°šæœªæ”¯æŒï¼Œè·³è¿‡
                                logger.warning(f"å°šæœªæ”¯æŒçš„IYUUç«™ç‚¹ï¼š{sid}")
                                continue
                            base_dict = {
                                "site_id": site.id,
                                "tid": tid,
                                "hash_string": info_hash,
                            }
                            current_list.append(base_dict)
                        except Exception as e:
                            logger.error(f' IYUU æ•°æ®: {info_hash} -tid: {tid} - iyuu: {sid} è§£æå‡ºé”™äº†ï¼š{e}')
                            logger.error(traceback.format_exc(5))
                            continue
                    repeat_info[hash_string] = current_list
                except Exception as e:
                    logger.error(f'è§£æ IYUU æ•°æ®å‡ºé”™äº†ï¼š{e}')
                    logger.error(traceback.format_exc(5))
            return CommonResponse.success(data=repeat_info)
        return CommonResponse.error(msg=res.get('msg'))
    except Exception as e:
        msg = f'ä»IYUUè·å–è¾…ç§æ•°æ®å¤±è´¥ï¼{e}'
        logger.error(msg)
        return CommonResponse.error(msg=msg)


def generate_notify_content(notice, status: SiteStatus):
    content = f""
    notice_content_enable = notice.get("notice_content_enable", True)
    if not notice_content_enable:
        return content
    notify_content_item = notice.get("notice_content_item", {
        'level': True,
        'bonus': True,
        'per_bonus': True,
        'score': True,
        'ratio': True,
        'seeding_vol': True,
        'uploaded': True,
        'downloaded': True,
        'seeding': True,
        'leeching': True,
        'invite': True,
        'hr': True
    })

    data = {
        'level': status.my_level,
        'bonus': status.my_bonus,
        'per_bonus': status.bonus_hour,
        'score': status.my_score,
        'ratio': status.ratio,
        'seeding_vol': FileSizeConvert.parse_2_file_size(status.seed_volume),
        'uploaded': FileSizeConvert.parse_2_file_size(status.uploaded),
        'downloaded': FileSizeConvert.parse_2_file_size(status.downloaded),
        'seeding': status.seed,
        'leeching': status.leech,
        'invite': status.invitation,
        'hr': status.my_hr
    }
    chinese_key = {
        'level': 'ç­‰çº§',
        'bonus': 'é­”åŠ›',
        'per_bonus': 'æ—¶é­”',
        'score': 'åšç§ç§¯åˆ†',
        'ratio': 'åˆ†äº«ç‡',
        'seeding_vol': 'åšç§é‡',
        'uploaded': 'å·²ä¸Šä¼ ',
        'downloaded': 'å·²ä¸‹è½½',
        'seeding': 'åšç§ä¸­',
        'leeching': 'ä¸‹è½½ä¸­',
        'invite': 'é‚€è¯·',
        'hr': 'H&R',
    }

    content += " ".join([f"{chinese_key[key]}ï¼š{data[key]}" for key in data if notify_content_item.get(key, True)])

    return content


def sht_reply(session, host: str, cookie: str, user_agent, message: str, fid: int = 95):
    """
    å›å¸–
    :param session:
    :param message: å›å¸–å†…å®¹
    :param host:
    :param cookie:
    :param user_agent:
    :param fid: æ¿å— ID
    :return:
    """
    # è®¿é—®ç»¼åˆé¡µé¢
    zonghe_url = f'{host}/forum.php?mod=forumdisplay&fid={fid}'
    logger.info(f"å½“å‰ç½‘å€ï¼š{zonghe_url}")
    response = session.get(
        url=zonghe_url,
        headers={
            "User-Agent": user_agent,
            'Cookie': cookie,
        },
        # cookies=cookie
    )
    logger.debug(f'å¸–å­åˆ—è¡¨ï¼š{response.text}')

    tid_pattern = r'normalthread_(\d+)'
    matches = re.findall(tid_pattern, response.text)
    tid = random.choice(matches)
    page_url = f'{host}/forum.php?mod=viewthread&extra=page%3D1&tid={tid}'
    logger.info(f"å½“å‰ç½‘å€ï¼š{page_url}")
    page_response = session.get(
        url=page_url,
        headers={
            "User-Agent": user_agent,
            'Cookie': cookie,
        },
        # cookies=cookie
    )
    logger.debug(f'å¸–å­è¯¦æƒ…é¡µï¼š{response.text}')

    action_pattern = r'id="fastpostform" action="(.*?)"'
    action_url = re.search(action_pattern, page_response.text).group(1).replace('amp;', '')
    submit_data = {
        "message": message,
        'usesig': '',
        'subject': '',
        'file': '',
    }
    form_object = etree.HTML(response.content.decode("utf-8")).xpath('//form[@id="fastpostform"]')
    # print(etree.tostring(form_object[0]).decode("utf-8"))

    for input in form_object[0].xpath('.//input[@type="hidden"]'):
        submit_data[input.xpath('./@name')[0]] = input.xpath('./@value')[0]

    url = f'{host}/{action_url}&inajax=1'
    logger.info(f"å½“å‰ç½‘å€ï¼š{url}")
    logger.info(f"æäº¤æ•°æ®ï¼š{submit_data}")
    headers = {
        'User-Agent': user_agent,
        'Referer': page_url,
        'Cookie': cookie,
        'Accept': '*/*',
        'Host': 'jq2t4.com',
        'Connection': 'keep-alive',
    }
    action_response = session.post(
        url=url,
        headers=headers,
        # cookies=cookie,
        data=submit_data,
    )
    logger.debug(f'å›å¸–ï¼š{action_response.text}')
    if action_response.status_code == 200 and action_response.text.find("å›å¤å‘å¸ƒæˆåŠŸ") > 0:
        logger.info("å›å¸–å®Œæˆï¼")
        return CommonResponse.success(msg='å›å¤å‘å¸ƒæˆåŠŸ')
    else:
        logger.info("å›å¸–å‡ºé”™å•¦ï¼")
        return CommonResponse.error(msg='å›å¸–å‡ºé”™å•¦ï¼')


def sht_sign(host, username, password, cookie, user_agent, message: str, fid: int = 95):
    try:
        cookies_dict = cookie2dict(cookie)
        # ç™»å½•ç•Œé¢URL
        login_ui_url = f'{host}/member.php?mod=logging&action=login&infloat=yes&handlekey=login&ajaxtarget=fwin_content_login'
        logger.info(login_ui_url)
        # åˆ›å»ºè¯·æ±‚å¯¹è±¡
        session = requests.Session()
        # æ‰“å¼€ç™»å½•ç•Œé¢
        response = session.get(
            url=login_ui_url,
            headers={
                "User-Agent": user_agent,
                "Referer": f'{host}/forum.php',
            },
            cookies=cookies_dict
        )
        logger.debug(response.content.decode('utf8'))
        # æ£€æµ‹åˆ°ç­¾åˆ°é“¾æ¥
        # pattern = r'<!\[CDATA\[(.*?)\]\]>'
        # match = re.search(pattern, response.content.decode('utf8'), re.DOTALL)
        # html_code = match.group(1)
        html_code = response.content.decode('utf8').replace('<?xml version="1.0" encoding="utf-8"?>', '').replace(
            '<root><![CDATA[', '').replace(']]></root>', '')
        check_login = etree.HTML(html_code).xpath('//a[@href="plugin.php?id=dd_sign:index"]')
        logger.info(f'Cookieæœ‰æ•ˆæ£€æµ‹ï¼šç­¾åˆ°é“¾æ¥å­˜åœ¨æ•°é‡ {len(check_login)}')
        # å¦‚æœæ£€æµ‹åˆ°ç­¾åˆ°é“¾æ¥ï¼Œåˆ™ç›´æ¥ä½¿ç”¨Cookieï¼Œå¦åˆ™é‡æ–°è·å–Cookie
        if not check_login or len(check_login) <= 0:
            logger.info(f'Cookieå¤±æ•ˆï¼Œé‡æ–°è·å–')
            # è§£æç™»å½•ç•Œé¢æ•°æ®ï¼Œè·å–formhashä¸loginhash
            html_object = etree.HTML(response.content.decode('utf8')[55:-10])
            # è·å–formè¡¨å•å¯¹è±¡
            form = html_object.xpath('//form')[0]
            # è·å–æäº¤é“¾æ¥
            login_action_link = form.xpath('@action')[0]
            logger.info(login_action_link)
            # è§£æç›¸å…³å­—æ®µ
            fields = form.xpath('.//input[@type="hidden"]')

            form_data = {
                "formhash": '',
                "referer": f'{host}/forum.php',
                "username": username,
                "password": password,
                "cookietime": 2592000
            }
            # è¾“å‡ºéœ€è¦å¡«å†™çš„å­—æ®µåå’Œå€¼
            for field in fields:
                name = field.get('name')
                value = field.get('value', '')
                form_data[name] = value
                logger.info(f"å­—æ®µå: {name}, å€¼: {value}")

            logger.info(f"ç™»å½•å‚æ•°ï¼š{form_data}")
            # ç™»å½•
            login_action_url = f'{host}{login_action_link}'
            login_response = session.post(
                url=login_action_url,
                data=form_data,
                headers={
                    "User-Agent": user_agent,
                    'referer': f"{host}/forum.php",
                },
                cookies={
                    '_safe': 'vqd37pjm4p5uodq339yzk6b7jdt6oich'
                }
            )
            logger.debug(f"ç™»å½•åé¦ˆï¼š{login_response.content.decode('utf8')}")
            cookies_dict = session.cookies.get_dict()
            msg = f"æ–°è·å–çš„Cookieï¼š{cookies_dict}"
            logger.info(msg)
            send_text(message=msg, title='è¯·åŠæ—¶æ›´æ–°98Cookie!')
        # æ£€æµ‹ç­¾åˆ°ä¸å¦
        check_sign_url = f'{host}/plugin.php?id=dd_sign:index'
        check_sign_response = session.get(
            url=check_sign_url,
            headers={
                "User-Agent": user_agent,
                "Referer": f'{host}/forum.php',
            },
            cookies=cookies_dict,
        )
        check_sign = etree.HTML(check_sign_response.content.decode('utf8')).xpath('//a[contains(text(),"ä»Šæ—¥å·²ç­¾åˆ°")]')
        if not check_sign or len(check_sign) <= 0:
            # å›å¸–
            reply = sht_reply(session=session, host=host, cookie=cookie, user_agent=user_agent, message=message,
                              fid=fid)
            logger.info(reply.msg)
            if reply.code != 0:
                return reply

            # æ‰“å¼€ç­¾åˆ°ç•Œé¢
            sign_ui_url = f'{host}/plugin.php?id=dd_sign&mod=sign&infloat=yes&handlekey=pc_click_ddsign&inajax=1&ajaxtarget=fwin_content_pc_click_ddsign'
            # è·å–idhash
            sign_response = session.get(
                url=sign_ui_url,
                headers={
                    "User-Agent": user_agent,
                    'referer': f"{host}/plugin.php?id=dd_sign:index",
                },
                cookies=cookies_dict,
            )
            logger.info(f'ç­¾åˆ°ç•Œé¢: {sign_response.content.decode("utf8")}')
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å­—æ®µ
            match = re.compile(
                # r'signhash=(.+?)".*name="formhash" value="(\w+)".*name="signtoken" value="(\w+)".*secqaa_(.+?)\"',
                r'signhash=(.+?)".*name="formhash" value="(\w+)".*secqaa_(.+?)\"',
                re.S)
            # signhash, formhash, signtoken, idhash = re.findall(match, sign_response.content.decode('utf8'))[0]
            signhash, formhash, idhash = re.findall(match, sign_response.content.decode('utf8'))[0]
            logger.info(f'ç­¾åˆ°ç•Œé¢å‚æ•°: \né“¾æ¥: {signhash} \n'
                        f' formhash: {formhash} \n signtoken:{None} \n idhash: {idhash}\n')
            # è·å–è®¡ç®—é¢˜
            calc_ui_url = f'{host}/misc.php?mod=secqaa&action=update&idhash={idhash}&{round(random.uniform(0, 1), 16)}'
            calc_response = session.get(
                url=calc_ui_url,
                headers={
                    "User-Agent": user_agent,
                    'referer': f"{host}/plugin.php?id=dd_sign:index",
                },
                cookies=cookies_dict,
            )
            logger.debug(f'è®¡ç®—é¢˜: {calc_response.content.decode("utf8")}')
            # è§£æç­¾åˆ°æ•°æ®
            pattern = r'(\d+\s*[-+*/]\s*\d+)'
            match = re.search(pattern, calc_response.content.decode('utf8'))
            logger.info(f'è§£æå‡ºçš„è®¡ç®—é¢˜: {match.group(0)}')
            calc_result = eval(match.group(1))
            logger.info(f'è®¡ç®—ç»“æœ: {calc_result}')
            # æ ¡éªŒç­¾åˆ°è®¡ç®—ç»“æœ
            calc_check_url = f'{host}/misc.php?mod=secqaa&action=check&inajax=1&modid=&idhash={idhash}&secverify={calc_result}'
            logger.info(f"ç­¾åˆ°æ£€æµ‹é“¾æ¥ï¼š{calc_check_url}")
            calc_check_response = session.get(
                url=calc_check_url,
                headers={
                    "User-Agent": user_agent,
                    'referer': f"{host}/plugin.php?id=dd_sign:index",
                },
                cookies=cookies_dict,
            )
            logger.debug(f"ç­¾åˆ°æ ¡éªŒç»“æœ: {calc_check_response.content.decode('utf8')}")
            if 'succeed' in calc_check_response.content.decode('utf8'):
                # å‘é€ç­¾åˆ°è¯·æ±‚
                sign_form_data = {
                    "formhash": formhash,
                    "signtoken": None,
                    "secqaahash": idhash,
                    "secanswer": calc_result,
                }
                sign_post_url = f'{host}/plugin.php?id=dd_sign&mod=sign&signsubmit=yes&handlekey=pc_click_ddsign&signhash={signhash}&inajax=1'
                logger.info(f"ç­¾åˆ°é“¾æ¥: {sign_post_url}")
                sign_response = session.post(
                    url=sign_post_url,
                    headers={
                        "User-Agent": user_agent,
                        'referer': f"{host}/plugin.php?id=dd_sign:index",
                    },
                    cookies=cookies_dict,
                    data=sign_form_data,
                )
                logger.debug(f"ç­¾åˆ°ç»“æœé¡µï¼š{sign_response.content.decode('utf8')}")
                match = re.search(r"showDialog\('([^']*)'", sign_response.content.decode('utf8'))
                result = match.group(1)
                logger.info(f'æœ¬æ¬¡ç­¾åˆ°ï¼š{result}')
            elif 'å·²ç»ç­¾åˆ°è¿‡å•¦ï¼Œè¯·æ˜å¤©å†æ¥ï¼' in sign_response.content.decode('utf8'):
                result = f't98å·²ç»ç­¾åˆ°è¿‡å•¦ï¼è¯·ä¸è¦é‡å¤ç­¾åˆ°ï¼'
            else:
                result = f't98ç­¾åˆ°å¤±è´¥!è¯·æ£€æŸ¥ç½‘é¡µï¼!'
        else:
            result = f't98å·²ç»ç­¾åˆ°è¿‡å•¦ï¼è¯·ä¸è¦é‡å¤ç­¾åˆ°ï¼'
            logger.info(result)
        # æ£€æŸ¥å½“å‰ç§¯åˆ†ä¸é‡‘å¸
        credit_url = f'{host}/home.php?mod=spacecp&ac=credit&op=base'
        credit_response = session.get(
            url=credit_url,
            headers={
                "User-Agent": user_agent,
                'referer': f"{host}/plugin.php?id=dd_sign:index",
            },
            cookies=cookies_dict,
        )
        logger.debug(f'ç§¯åˆ†é‡‘å¸é¡µé¢è¯¦æƒ…ï¼š{credit_response.content.decode("utf8")}')

        pattern = re.compile(
            r'(é‡‘é’±:\s)*<\/em>(\d+)|(è‰²å¸:\s)*<\/em>(\d+)|(ç§¯åˆ†:\s)*<\/em>(\d+)|(è¯„åˆ†:\s)*<\/em>(\d+)',
            re.S)
        matches = re.findall(pattern, credit_response.content.decode("utf8"))
        info = 'ï¼Œ'.join([''.join(match) for match in matches])
        logger.info(f'ç§¯åˆ†é‡‘å¸è¯¦æƒ…: {info}')
        msg = f"æœ¬æ¬¡ç­¾åˆ°:{result}\nç§¯åˆ†é‡‘å¸è¯¦æƒ…: {info}"

        # è·å–å½“å‰æ—¶é—´
        now = datetime.now()
        # è®¡ç®—å½“å¤©ç»“æŸçš„æ—¶é—´
        end_of_day = now.replace(hour=23, minute=59, second=59)
        # è®¡ç®—å½“å‰æ—¶é—´åˆ°å½“å¤©ç»“æŸçš„æ—¶é—´é—´éš”
        expiration = end_of_day - now
        cache.set(f"t98_sign_in_state", True, expiration.seconds)
        return CommonResponse.success(msg=msg)

    except Exception as e:
        msg = f'98ç­¾åˆ°å¤±è´¥ï¼š{e}'
        logger.info(traceback.format_exc(8))
        return CommonResponse.error(msg=msg)


def sign_ssd_forum(cookie, user_agent, todaysay):
    try:
        logger.info('SSDForumå¼€å§‹ç­¾åˆ°')
        # è®¿é—®ç­¾åˆ°é¡µ
        sign_url = 'https://ssdforum.org/plugin.php?id=dsu_paulsign:sign'
        sign_response = requests.get(
            url=sign_url,
            headers={
                'User-Agent': user_agent,
                'Referer': 'https://ssdforum.org/',
            },
            cookies=cookie2dict(cookie),
        )
        logger.debug(f'ç­¾åˆ°é¡µHTMLï¼š{sign_response.text}')
        if sign_response.status_code != 200:
            return CommonResponse.error(msg=f'SSDForumç­¾åˆ°å¤±è´¥:{sign_response.status_code}')
        html_object = etree.HTML(sign_response.content.decode('gbk'))
        sign_check = html_object.xpath('//div[@class="c"]/text()')
        logger.info(f"ç­¾åˆ°æ£€æµ‹ï¼š{sign_check}")
        sign_text = ''
        if not sign_check or len(sign_check):
            logger.info(f"ç­¾åˆ°æ£€æµ‹ï¼š{len(sign_check)}")
            # action_url = html_object.xpath('//form[@id="qiandao"]/@action')
            formhash = ''.join(html_object.xpath('//form[@id="qiandao"]/input[@name="formhash"]/@value'))
            # è·å–å¹¶ç”Ÿæˆç­¾åˆ°å‚æ•°
            qdxq_options = ['kx', 'ng', 'ym', 'wl', 'nu', 'ch', 'fd', 'yl', 'shuai']
            form_data = {
                'formhash': formhash,
                'qdxq': random.choice(qdxq_options),  # replace with the desired value
                'qdmode': '1',  # replace with the desired value
                'todaysay': todaysay,  # replace with the desired value
            }
            logger.info(f'ç­¾åˆ°å‚æ•°ï¼š{form_data}')
            # å‘é€ç­¾åˆ°è¯·æ±‚
            sign_in_url = 'https://ssdforum.org/plugin.php?id=dsu_paulsign:sign&operation=qiandao&infloat=1'
            sign_in_response = requests.post(
                url=sign_in_url,
                headers={
                    'User-Agent': user_agent,
                    'Referer': 'https://ssdforum.org/',
                },
                cookies=cookie2dict(cookie),
                data=form_data,
            )
            # è§£æç­¾åˆ°åé¦ˆ
            logger.debug(f'ç­¾åˆ°åé¦ˆï¼š{sign_in_response.text}')
            sign_text = ''.join(etree.HTML(sign_in_response.content.decode('gbk')).xpath('//div[@class="c"]/text()'))
        else:
            sign_text = 'ä»Šæ—¥å·²ç­¾åˆ°'
            logger.info(sign_text)
        # è·å–å½“å‰æ—¶é—´
        now = datetime.now()
        # è®¡ç®—å½“å¤©ç»“æŸçš„æ—¶é—´
        end_of_day = now.replace(hour=23, minute=59, second=59)
        # è®¡ç®—å½“å‰æ—¶é—´åˆ°å½“å¤©ç»“æŸçš„æ—¶é—´é—´éš”
        expiration = end_of_day - now
        cache.set(f"sign_ssd_forum_state", True, expiration.seconds)
        sign_response = requests.get(
            url=sign_url,
            headers={
                'User-Agent': user_agent,
                'Referer': 'https://ssdforum.org/',
            },
            cookies=cookie2dict(cookie),
        )
        logger.debug(f"ç­¾åˆ°é¡µï¼š{sign_response.text}")
        sign_title_rule = '//div[@class="mn"]/h1[1]/text()'
        sign_content_rule = '//div[@class="mn"]/p//text()'
        title = etree.HTML(sign_response.content.decode('gbk')).xpath(sign_title_rule)
        content = etree.HTML(sign_response.content.decode('gbk')).xpath(sign_content_rule)
        result = f'{sign_text}ã€‚{"".join(title)} {"".join(content)}'
        logger.info(f'SSDForumç­¾åˆ°ç»“æœ: {result}')
        return CommonResponse.success(msg=result)
    except Exception as e:
        msg = f'SSDForumç­¾åˆ°å¤±è´¥ï¼Œ{e}'
        logger.error(traceback.format_exc(5))
        logger.error(msg)
        return CommonResponse.error(msg=msg)


def cnlang_sign(
        username,
        cookie,
        host,
        user_agent
):
    try:
        s = requests.session()
        headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept - Encoding': 'gzip, deflate, br',
            'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
            'cache-control': 'max-age=0',
            'Upgrade-Insecure-Requests': '1',
            'Host': host,
            'Cookie': cookie,
            'User-Agent': user_agent
        }

        # è®¿é—®Pcä¸»é¡µ
        logger.info(host)
        user_info = s.get(
            'https://' + host + '/dsu_paulsign-sign.html?mobile=no',
            headers=headers,
            cookies=cookie2dict(cookie)
        ).text
        user_name = re.search(r'title="è®¿é—®æˆ‘çš„ç©ºé—´">(.*?)</a>', user_info)

        # è§£æ HTML é¡µé¢
        # soup = BeautifulSoup(html, 'html.parser')
        tree = etree.HTML(user_info)

        # æ‰¾åˆ° name ä¸º formhash çš„ input æ ‡ç­¾
        # formhash_input = soup.find('input', {'name': 'formhash'})
        formhash_value = ''.join(tree.xpath('//input[@name="formhash"]/@value'))

        # ä» input æ ‡ç­¾ä¸­æå– formhash çš„å€¼
        # formhash_value = re.search(r'value="(.+?)"', str(formhash_input)).group(1)

        logger.info("formhashï¼š" + formhash_value)
        # éšæœºè·å–å¿ƒæƒ…
        xq = s.get('https://v1.hitokoto.cn/?encode=text').text
        # ä¿è¯å­—æ•°ç¬¦åˆè¦æ±‚
        logger.info("æƒ³è¯´çš„è¯ï¼š" + xq)
        while (len(xq) < 6 | len(xq) > 50):
            xq = s.get('https://v1.hitokoto.cn/?encode=text').text
            logger.info("æƒ³è¯´çš„è¯ï¼š" + xq)
        if user_name:
            logger.info("ç™»å½•ç”¨æˆ·åä¸ºï¼š" + user_name.group(1))
            logger.info("ç¯å¢ƒç”¨æˆ·åä¸ºï¼š" + username)
        else:
            logger.info("æœªè·å–åˆ°ç”¨æˆ·å")
        if user_name is None or (user_name.group(1) != username):
            raise Exception("ã€å›½è¯­è§†ç•Œã€‘cookieå¤±æ•ˆ")
        # è·å–ç­¾åˆ°é“¾æ¥,å¹¶ç­¾åˆ°
        qiandao_url = 'plugin.php?id=dsu_paulsign:sign&operation=qiandao&infloat=1'

        # ç­¾åˆ°
        payload = dict(formhash=formhash_value, qdxq='kx', qdmode='1', todaysay=xq, fastreply='0')
        logger.info(f"form_data: {payload}")
        qdjg = s.post(
            'https://' + host + '/' + qiandao_url,
            headers=headers,
            data=payload,
            cookies=cookie2dict(cookie)
        ).text

        # soup = BeautifulSoup(html, 'html.parser')
        # div = soup.find('div', {'class': 'c'})  # æ‰¾åˆ° class ä¸º clashï¼Œid ä¸º c çš„ div
        # content = div.text  # è·å– div çš„æ–‡æœ¬å†…å®¹
        content = ''.join(etree.HTML(qdjg).xpath('//div[@class="c"]/text()'))

        logger.info(content)
        # è·å–å½“å‰æ—¶é—´
        now = datetime.now()
        # è®¡ç®—å½“å¤©ç»“æŸçš„æ—¶é—´
        end_of_day = now.replace(hour=23, minute=59, second=59)
        # è®¡ç®—å½“å‰æ—¶é—´åˆ°å½“å¤©ç»“æŸçš„æ—¶é—´é—´éš”
        expiration = end_of_day - now
        cache.set(f"cnlang_sign_state", True, expiration.seconds)
        # è·å–ç§¯åˆ†
        user_info = s.get(
            'https://' + host + '/home.php?mod=spacecp&ac=credit&showcredit=1&inajax=1&ajaxtarget=extcreditmenu_menu',
            headers=headers,
            cookies=cookie2dict(cookie)
        ).text
        current_money = re.search(r'<span id="hcredit_2">(\d+)</span>', user_info).group(1)
        log_info = f'clang ç­¾åˆ°ï¼š{content} å½“å‰å¤§æ´‹ä½™é¢ï¼š{current_money}'
        logger.info(log_info)
        # send("ç­¾åˆ°ç»“æœ", log_info)
        return CommonResponse.success(msg=log_info)
    except Exception as e:
        msg = f'clangç­¾åˆ°å¤±è´¥ï¼Œå¤±è´¥åŸå› : {e}'
        logger.error(msg)
        logger.error(traceback.format_exc(5))
        return CommonResponse.error(msg=msg)


def get_time_join(my_site, details_html):
    site = get_object_or_404(WebSite, id=my_site.site)
    mirror = my_site.mirror if my_site.mirror_switch else site.url
    try:
        if 'greatposterwall' in site.url or 'dicmusic' in site.url:
            logger.debug(details_html)
            details_response = details_html.get('response')
            stats = details_response.get('stats')
            my_site.time_join = stats.get('joinedDate')
            my_site.latest_active = stats.get('lastAccess')
            my_site.save()
        elif "m-team" in mirror:
            pass
        elif 'zhuque.in' in site.url:
            my_site.time_join = datetime.fromtimestamp(details_html.get(site.my_time_join_rule))
            my_site.save()
        else:
            logger.debug(f'æ³¨å†Œæ—¶é—´ï¼š{details_html.xpath(site.my_time_join_rule)}')
            if site.url in [
                'https://monikadesign.uk/',
                'https://pt.hdpost.top/',
                'https://reelflix.xyz/',
            ]:
                time_str = ''.join(details_html.xpath(site.my_time_join_rule))
                time_str = re.sub(u"[\u4e00-\u9fa5]", "", time_str).strip()
                time_join = datetime.strptime(time_str, '%b %d %Y')
                logger.debug(f'æ³¨å†Œæ—¶é—´ï¼š{time_join}')
                my_site.time_join = time_join
            elif site.url in [
                'https://hd-torrents.org/',
            ]:
                my_site.time_join = datetime.strptime(
                    ''.join(details_html.xpath(site.my_time_join_rule)).replace('\xa0', ''),
                    '%d/%m/%Y %H:%M:%S'
                )
            elif site.url in [
                'https://hd-space.org/',
            ]:
                my_site.time_join = datetime.strptime(
                    ''.join(details_html.xpath(site.my_time_join_rule)).replace('\xa0', ''),
                    '%B %d, %Y,%H:%M:%S'
                )
            elif site.url in [
                'https://jpopsuki.eu/',
            ]:
                my_site.time_join = datetime.strptime(
                    ''.join(details_html.xpath(site.my_time_join_rule)).replace('\xa0', ''),
                    '%b %d %Y, %H:%M'
                )
            elif site.url in [
                'https://www.torrentleech.org/',
            ]:
                my_site.time_join = dateutil.parser.parse(''.join(details_html.xpath(site.my_time_join_rule)))
            elif site.url in [
                'https://exoticaz.to/',
                'https://cinemaz.to/',
                'https://avistaz.to/',
            ]:
                time_str = ''.join(details_html.xpath(site.my_time_join_rule)).split('(')[0].strip()
                my_site.time_join = datetime.strptime(time_str, '%d %b %Y %I:%M %p')
            else:
                time_join = re.findall(r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}', ''.join(
                    details_html.xpath(site.my_time_join_rule)
                ).strip())
                my_site.time_join = ''.join(time_join)
            my_site.latest_active = datetime.now()
            my_site.save()
    except Exception as e:
        msg = f'ğŸ†˜ {site.name} æ³¨å†Œæ—¶é—´è·å–å‡ºé”™å•¦ï¼'
        logger.error(msg)
        logger.error(traceback.format_exc(3))


def sync_cookie_from_cookie_cloud(server: str, key: str, password: str):
    """
    åŒæ­¥ cookie
    :param server:
    :param key:
    :param password:
    :return:
    """
    try:
        helper = CookieCloudHelper(server=server, key=key, password=password)
        res = helper.download()
        if res.code != 0:
            return res
        website_list = WebSite.objects.all()
        msg_list = []
        count_created = 0
        count_updated = 0
        count_failed = 0
        for domain, cookie in res.data.items():
            try:
                website = website_list.filter(alive=True).get(url__contains=domain)
                mysite, created = MySite.objects.update_or_create(site=website.id, defaults={"cookie": cookie})

                if created:
                    mysite.nickname = website.name
                    mysite.save()
                    count_created += 1
                    msg = f'- {mysite.nickname} ç«™ç‚¹æ·»åŠ æˆåŠŸï¼\n'
                    logger.info(f'å¼€å§‹è·å– UIDï¼ŒPASSKEYï¼Œæ³¨å†Œæ—¶é—´')
                    try:
                        scraper = cloudscraper.create_scraper(browser={
                            'browser': 'chrome',
                            'platform': 'darwin',
                        })
                        response = scraper.get(
                            url=website.url + website.page_control_panel,
                            cookies=cookie2dict(cookie),
                        )
                        logger.debug(f'æ§åˆ¶é¢æ¿é¡µé¢ï¼š{response.text}')
                        html_object = etree.HTML(response.content)
                        mysite.user_id = ''.join(html_object.xpath(website.my_uid_rule)).split('=')[-1]
                        mysite.passkey = ''.join(html_object.xpath(website.my_passkey_rule))
                        get_time_join(mysite, html_object)
                        mysite.save()
                        logger.debug(f'uid:{mysite.user_id}')
                        logger.debug(f'passkey:{mysite.passkey}')
                    except Exception as e:
                        msg += f'è·å– UIDï¼ŒPASSKEYæˆ–æ³¨å†Œæ—¶é—´å¤±è´¥ï¼è¯·æ‰‹åŠ¨è·å–ï¼{e}'
                else:
                    msg = f'- {mysite.nickname} ç«™ç‚¹æ›´æ–°æˆåŠŸï¼\n'
                    count_updated += 1
                logger.info(msg)
                msg_list.append(msg)
            except Exception as e:
                logger.error(f'å°šä¸æ”¯æŒæˆ–æ­¤ç«™ç‚¹å·²å…³é—­ï¼š{domain} ')
                count_failed += 1
                continue
        msg = f'> æœ¬æ¬¡åŒæ­¥ä»»åŠ¡å…±æ·»åŠ ç«™ç‚¹ï¼š{count_created}, æ›´æ–°ç«™ç‚¹ï¼š{count_updated}, å¤±è´¥ç«™ç‚¹ï¼š{count_created}\n'
        logger.info(msg)
        msg_list.insert(0, msg)
        return CommonResponse.success(msg=''.join(msg_list))
    except Exception as e:
        return CommonResponse.error(msg=f'åŒæ­¥ Cookie å‡ºé”™å•¦ï¼{e}')


def push_torrents_to_sever(push_once: int):
    """
    ä¸Šä¼ æœ¬åœ°ç§å­åˆ°æœåŠ¡å™¨
    :param push_once: å•è¯ä¸Šä¼ æ•°æ®é‡
    :return:
    """
    try:
        # ä»æ•°æ®åº“è¯»å–hash ä¸ä¸ºç©ºçš„ç§å­
        torrents = TorrentInfo.objects.filter(pushed=False).exclude(hash_string__exact='')
        logger.info(f'å½“å‰å…±æœ‰ç¬¦åˆæ¡ä»¶çš„ç§å­ï¼š{len(torrents)}')
        if len(torrents) <= 0:
            return f"å½“å‰æ²¡æœ‰éœ€è¦æ¨é€çš„ç§å­ï¼"
        # æ¨é€åˆ°æœåŠ¡å™¨
        msg = []
        while torrents:
            chunks = torrents[:push_once]  # ä»æ¶ˆæ¯ä¸­æˆªå–æœ€å¤§é•¿åº¦çš„éƒ¨åˆ†
            try:
                torrent_list = [t.to_dict(exclude=['id']) for t in chunks]
                logger.debug(torrent_list[0])
                logger.debug(torrents)
                res = requests.post(
                    url=f"{os.getenv('REPEAT_SERVER')}/api/website/torrents/multiple",
                    json=torrent_list,
                    headers={
                        "content-type": "application/json",
                        "AUTHORIZATION": os.getenv("TOKEN"),
                        "EMAIL": os.getenv("DJANGO_SUPERUSER_EMAIL"),
                    }
                )

                # å·²å­˜æ¡£çš„ç§å­æ›´æ–°çŠ¶æ€ä¸º6==å·²æ¨é€åˆ°æœåŠ¡å™¨
                logger.info(res.text)
                if res.status_code == 200:
                    torrents = torrents[push_once:]
                    TorrentInfo.objects.filter(id__in=[t.id for t in chunks]).update(pushed=True)
                    msg.append(f"æˆåŠŸä¸Šä¼  {len(chunks)} æ¡ç§å­ä¿¡æ¯")
                else:
                    msg.append(f"{len(chunks)} æ¡ç§å­ä¿¡æ¯ä¸Šä¼ å¤±è´¥ï¼")
            except Exception as e:
                err_msg = f'æ¨é€ç§å­ä¿¡æ¯åˆ°æœåŠ¡å™¨å¤±è´¥ï¼{e}'
                logger.error(err_msg)
                msg.append(msg)
                logger.error(traceback.format_exc(5))
        logger.info(msg)
        return '\n'.join(msg)
    except Exception as e:
        msg = f'æ¨é€ç§å­ä¿¡æ¯åˆ°æœåŠ¡å™¨å¤±è´¥ï¼{e}'
        logger.error(msg)
        logger.error(traceback.format_exc(5))
        return msg


def calculate_expiry_time_from_string(time_str):
    """
    è§£æmteamå…è´¹æ—¶é—´
    :param time_str:
    :return:
    """

    def parse_remaining_time(time_str: str):
        time_str.replace("æ™‚", "æ—¶").replace("å¤©", "æ—¥")
        # æ£€æŸ¥æ˜¯å¦æ˜¯ "X æ—¥ X æ™‚" æ ¼å¼
        if "æ—¥" in time_str and "æ—¶" in time_str:
            days = int(re.search(r"(\d+)\s*æ—¥", time_str).group(1))
            hours = int(re.search(r"(\d+)\s*æ—¶", time_str).group(1))
            return days, hours, 0

        # æ£€æŸ¥æ˜¯å¦æ˜¯ "X æ™‚ X åˆ†" æ ¼å¼
        elif "æ—¶" in time_str and "åˆ†" in time_str:
            hours = int(re.search(r"(\d+)\s*æ—¶", time_str).group(1))
            minutes = int(re.search(r"(\d+)\s*åˆ†", time_str).group(1))
            return 0, hours, minutes

        # æ£€æŸ¥æ˜¯å¦æ˜¯ "X åˆ†" æ ¼å¼
        elif "åˆ†" in time_str:
            minutes = int(re.search(r"(\d+)\s*åˆ†", time_str).group(1))
            return 0, 0, minutes

        else:
            raise ValueError("æ— æ³•è§£ææ—¶é—´å­—ç¬¦ä¸²")

    def calculate_expiry_time(days: int, hours: int, minutes: int):
        # è·å–å½“å‰æ—¥æœŸå’Œæ—¶é—´
        current_time = datetime.now()

        # è®¡ç®—å‰©ä½™æ—¶é—´çš„æ—¶é—´å·®
        time_difference = timedelta(days=days, hours=hours, minutes=minutes)

        # è®¡ç®—åˆ°æœŸæ—¶é—´

        return current_time + time_difference

    # è§£æå­—ç¬¦ä¸²å¹¶è®¡ç®—åˆ°æœŸæ—¶é—´
    days, hours, minutes = parse_remaining_time(time_str)
    expiry_time = calculate_expiry_time(days, hours, minutes)

    return expiry_time
